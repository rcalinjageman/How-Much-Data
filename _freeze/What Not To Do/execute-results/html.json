{
  "hash": "dd69e645d3b9836edd50e4af572a1552",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"How *Not* to Get Your Sample Size\"\nbibliography: references.bib\nexecute: \n  freeze: auto\n---\n\n{{< video https://youtu.be/dwyYmGmdFHA >}}\n\n\n\n\n\n\nThe video above walks through some common but poor sample-size planning practices: run-and-check (backing into a sample-size by chasing statistical significance) and uncritically adopting previous sample sizes.\n\nIn the explorations below you can dig into why run-and-check is so bad, and you can also explore the suprising fact that *p* \\< .05 does not mean you have an adequate sample size.\n\n## Exploration: Don't Run-and-Check!\n\nA common approach to sample-size determination is Run-and-Check: you run a small batch of samples, check for significance, and then keep adding samples until you either get what you want or run out of time/resources. Sometimes run-and-check is conducted over people/labs – keep assigning the same project to different trainees until someone with 'good hands' gets it to work (sometimes that really is good hands, but it can also easily be capitalization chance!).\n\nLet's examine why run-and-check is so dangerous to good science. We'll simulate data for a two-group experiment, with the simulation set so that there is no true effect in the population. We'll start with *n* = 3/group, then check. If the results are significant, we stop, otherwise we add 1 sample/group and check again, and so on, to a limit of *n* = 30 group. In this scenario, any statistically significant finding is a false positive. We'll run 10,000 simulations and check the false-positive rate.\n\nThe code for the simulation is below. Before you scroll down to its output, what is your guess for the false positive rate for this run-and-check approach?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# If needed, install needed libraries\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif (!require(\"gganimate\")) install.packages(\"gganimate\")\nif (!require(\"gifski\")) install.packages(\"gifski\")\n\n\n# Load needed libraries\nlibrary(statpsych)\nlibrary(ggplot2)\nlibrary(gganimate)\n\n\n# Simulation parameters\nn_initial <- 3\nn_add_per_round <- 1\nn_to_give_up <- 30\ntrue_effect_size <- 0\ntotal_sims <- 1000\n\n# Initialize variables for results\nfalse_positives <- 0\nrandc_res <- data.frame(p = NA, n = NA, sample = NA, eventually_significant = NA)\nresults_row <- 0\n\n# simulation \nfor (i in 1:total_sims) {\n  \n  # simulate data at initial sample size for both groups and do t.test\n  n_current <- n_initial\n  g1 <- rnorm(n_current, 0, 1)\n  g2 <- rnorm(n_current, 0 + true_effect_size, 1)\n  res <- t.test(g1, g2)$p.value\n  results_row <- results_row + 1\n  \n  # store the results for plotting\n  randc_res[results_row, ] <- c(\n    res,\n    n_current,\n    i,\n    FALSE\n  )\n  \n  # if not significant and not at n_to_give_up yet, keep going\n  while (res >= .05 & n_current < n_to_give_up) {\n    g1 <- c(g1, rnorm(n_add_per_round, 0, 1))\n    g2 <- c(g2, rnorm(n_add_per_round, 0 + true_effect_size, 1))\n    n_current <- n_current + n_add_per_round\n    results_row <- results_row + 1\n    res <- t.test(g1, g2)$p.value\n    \n    randc_res[results_row, ] <- c(\n      res,\n      n_current,\n      i,\n      FALSE\n    )\n  }\n  \n  # If the final result is significnt, that's a false positive (assuming true_effect_size == 0)\n  if (res < .05) {\n    false_positives <- false_positives + 1\n    randc_res[randc_res$sample == i, ]$eventually_significant <- TRUE\n  }\n  \n}\n\n\nprint(\n  paste(\n    \"Simulated\", total_sims, \n    \"two-group studies with a true mean difference of\", true_effect_size,\n    \"where sample sizes started off with\", n_initial, \"/group\",\n    \"and if not significant added\", n_add_per_round, \"/group and re-checked,\",\n    \"up to significance or a max sample size of\", n_to_give_up, \".\"\n  )\n)\nprint(\"Proportion of significant findings\")\nprint(false_positives / total_sims)\n```\n:::\n\n\n\n\n### Simulation Results: False-Positive Rate\n\nThe code simulated 1000 two-group studies with a true mean difference 0 where sample sizes started off with 3/group and if not significant added 1/group up to significance or a max sample size of 30.\n\nUnder this scenario, the proportion of false positives is: 23\\.8%.\n\nThis scenario is a bit extreme, but it makes it clear that run-and-check is not a good method for determining sample size.\n\n### Simulation Results: The Deceptive Dance of the p Value\n\nTo make this even more clear, we're going to plot the p values as sample sizes are added, cycling through the first 10 simulations(@fig-run_and_check). What you'll notice is that the p values \"feel\" like they are following trajectories, swining higher or lower. This can give the researcher using run-and-check the feeling that they're on to something, and that they are justified adding more samples, or worse, adjusting their analyses to coax the p value below .05.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nrandc_res$animate_frame <- 1:nrow(randc_res)\nrandc_res$eventually_significant <- as.factor(randc_res$eventually_significant)\nrandc_res$sample_as_factor <- as.factor(randc_res$sample)\n\npdance_plot <- ggplot(data = randc_res[randc_res$sample < 5 , ], aes(x = n, y = p, group = sample, colour = eventually_significant))\npdance_plot <- pdance_plot + ylab(\"Obtained p value\")\npdance_plot <- pdance_plot + xlab(\"Sample size\")\npdance_plot <- pdance_plot + geom_point(size = 3)\npdance_plot <- pdance_plot + theme_classic() + theme(legend.position = \"none\")\npdance_plot <- pdance_plot + geom_hline(yintercept = 0.05, linetype = \"dotted\")\npdance_plot <- pdance_plot + scale_colour_manual(values = c(\"1\" = \"red\", \"0\" = \"dodgerblue1\"))\npdance_plot <- pdance_plot + ylim(c(0, 1))\npdance_plot <- pdance_plot + xlim(c(n_initial, n_to_give_up))\npdance_plot <- pdance_plot + transition_reveal(animate_frame, keep_last = FALSE)\nanimate(pdance_plot, fps = 2, renderer = gifski_renderer(\"run_and_check.gif\"))\n```\n\n::: {.cell-output-display}\n![Evolution of p values as samples are added but no true effect](What-Not-To-Do_files/figure-html/fig-run_and_check-1.gif){#fig-run_and_check}\n:::\n:::\n\n\n\n\nThus, it's not just the instinsic false-positive rate of run-and-check that is dangerous but also the false impression it can give that might license analytic flexibility towards erroneous results.\n\n# Exploration: The Perils of Inadequate Samples, Even When *p* \\< .05\n\nDespite calls for sample-size plans, many researchers seem to avoid sample-size planning or conduct *pro forma* plans to evade the requirement. Why? One reason seems to be the misconception that if your lab gets *p* \\< .05, then you have nothing to worry about. Here's an editorial where this sentiment was very clearly expressed:\n\n> However, somehow, journals have taken to asking how our animal studies were powered − and here's the point: If the results are statistically significant then, indeed, our study is appropriately powered. Sometimes, I'm not sure that the editors who insist on this information understand this.\n>\n> [@mole2017]\n\nThis attitude is seductive, but ultimately incorrect [@mole2018]. It's important to think this through for yourself so you can understand why inadequate samples are so problematic.\n\nImagine you are measuring the effect of maternal separation on gene expression in the hippocampus. You'll have two groups: Control and Separated. You'll use qPCR to measure expression of *egr1*, a constituitvely-expressed gene that plays an important role in learning and memory.\n\nWithout a sample-size plan, you'd likely rely on the previous literature or lab practices to set a sample size. In qPCR experiments like this one, *n* = 6/group is pretty common, so that's what you decide to go with.\n\nLet's suppose that in this case, your research hypothesis is absolutely correct: maternal separation *does* produce a large and robust increase in *egr1* expression. We'll set the effect size at 1 standard deviation (Cohen's *d* = 1; see the section on effect sizes if you're not sure what this means).\n\nThis sounds like an ideal situation: the researcher has a hypothesis that is actually correct! Let's see, though, what happens when this true hypothesis is investigated with *n* = 6/group.\n\n### Simulation\n\nIn the simulation below, we randomly generate gene-expression data for the Control and Separated groups, drawing the data from distributions that are 1 standard deviation apart. With each draw we then run a t-test and check to see if it is statistically significant or not. Given that the researcher's hypothesis is true, we might expect this experiment to always \"work\", regularly producing *p* \\< .05. In fact, though, it does not.!\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# If needed, install needed libraries\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif (!require(\"gganimate\")) install.packages(\"gganimate\")\nif (!require(\"gifski\")) install.packages(\"gifski\")\n\n\n# Load needed libraries\nlibrary(statpsych)\nlibrary(ggplot2)\nlibrary(gganimate)\n\n\n# Simulation parameters\neffect_size_in_sds <- 1\nsamples_per_group <- 6\nalpha <- 0.05\nsimcount <- 10000\n\n# Initialize variables for results\nsigfindings <- 0\nall_res <- data.frame(d = NA, p = NA, distortion = NA, sample = NA, CIlength = NA)\n\n# Now do the simulations\nfor (i in 1:simcount) {\n  # Control group \n  control <- rnorm(samples_per_group, 0, 1)\n  # Experimental group\n  experimental <- rnorm(samples_per_group, 0, 1) + effect_size_in_sds\n  # Analyze with a t-test\n  myres <- t.test(control, experimental)\n  # Increase counter if a significant finding\n  if (myres$p.value < alpha) sigfindings <- sigfindings + 1\n  # Express this finding in Cohen's d\n  myd <- statpsych::ci.stdmean2(\n    alpha = alpha,\n    m2 = mean(control),\n    m1 = mean(experimental),\n    sd2 = sd(control),\n    sd1 = sd(experimental),\n    n2 = length(control),\n    n1 = length(experimental)\n  )\n  \n  # Store the result of this sample\n  all_res[i, ] <- c(\n    myd[1, 2],\n    myres$p.value,\n    myd[1, 2] / effect_size_in_sds,\n    i,\n    myd[1, 5] - myd[1, 4]\n  )\n  \n}\n\nprint(paste(\"Simulated\", simcount, \"2-group experiments with n =\", samples_per_group, \"/group and a true effect of\", effect_size_in_sds, \"standard deviations.\"))\nprint(\"Proportion of statistically significant findings (p < .05):\")\nprint(sigfindings/simcount)\nprint(\"Typical margin of error:\")\nprint(mean(all_res$CIlength/2))\n```\n:::\n\n\n\n\n### Simulation Results: Power\n\nThe code above simulates 10000 2-group experiments with n = 6 and a true effect of 1 standard deviations.\n\nUnder this scenario, where there is a large effect, only 33\\.3% of samples yield statistically significant results (p \\< .05).\n\nThis sample-size is inadequate because the typical margin of error is 1\\.37\n\nAs you can see, only about 33% of experiments yield *p* \\< .05 *even though there is a real and substantial effect to be observed!* Why so low? Well, consider that statistical tests examine if the difference observed (signal) is substantially greater than expected sampling error (noise). With the sample-size we selected, though, expected sampling error is large: in fact, the margin of error in these studies is typically \\~1.37 standard deviations, far larger than the signal it would be reasonable to expect!\n\nIf the noise in this type of study is bigger than the signal, how do any of the experiments still turn out to be statistically significant? This occurs when sampling error breaks in just the right way to *distort* the effect–to make it seem much bigger than it really is, so that, for that misleading sample at least, the effect observed is substantially larger than the real truth. Uh oh! That's right, in these cases *p* \\< .05 can only occur *by mis-characterizing the actual truth*.\n\n### Simulation Results: Observed Effect Sizes\n\nLet's see this in more detail. @fig-effect_size_inlfation plots the effect observed in each study. The line at 1 standard deviation represents the true effect. Because of sampling error, studies \"dance\" around this truth – some get a bit too large of an effect, others a bit too small. Notice, though, the coloring, which represents statistical significance of each simulated experiment. Most of the dots are blue, not significant – that's the low power of the experiment. But note that the statistically significant findings (the red dots) all studies that radically *over-estimated* the real truth:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntime_plot <- ggplot(data = all_res[all_res$sample < 200, ], aes(x=sample, y = d, colour = (p < alpha)))\ntime_plot <- time_plot + ylab(\"Standardized Mean Difference in egr Expression (Experimental vs. Control)\")\ntime_plot <- time_plot + geom_point(aes(group = sample))\ntime_plot <- time_plot + theme_classic() + theme(legend.position = \"none\")\ntime_plot <- time_plot + scale_colour_manual(values = c(\"TRUE\" = \"red\", \"FALSE\" = \"dodgerblue1\"))\ntime_plot <- time_plot + geom_hline(yintercept = 0, linetype = \"dotted\")\ntime_plot <- time_plot + geom_hline(yintercept = 1)\ntime_plot <- time_plot + transition_reveal(sample)\nanimate(time_plot, fps = 6, renderer = gifski_renderer(\"d_over_time_updated.gif\"))\n```\n\n::: {.cell-output-display}\n![Observed effect sizes by stat significance status.](What-Not-To-Do_files/figure-html/fig-effect_size_inflation-1.gif){#fig-effect_size_inflation}\n:::\n:::\n\n\n\n\n### Simulation Results: Effect-Size Inflation Among Significant Results\n\nHow bad is this over-estimation? Let's find out:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(\"In this scenario, the true effect size is 1 standard deviations\")\nprint(\"Across all stimulated studies, the average effect size observed is:\")\nmean(all_res$d)\nprint(\"But for the studies that reached statistical significance, the average effect size observed is:\")\nmean(all_res[all_res$p < .05, ]$d)\n```\n:::\n\n\n\n\n-   In this scenario, the true effect size is: 1 standard deviations\n\n-   Across all stimulated studies, the average effect size observed is: 0\\.99\n\n-   But for the studies that reached statistical significance, the average effect size observed is: 1\\.69\n\nWow! The statistically-significant effects over-state the truth by over 70%. Below is a representation of the true effect and the effect that would be reported through the statistical-significance filter:\n\n![True Effect Size](images/rpsychologist-cohend.svg){width=\"476\"}\n\n![Observed Effect Size when p \\< .05.\\\n(hat-tip to the R Psychologist for the figures; <https://rpsychologist.com/cohend/>)](images/rpsychologist-cohend(1).svg){fig-alt=\"Figure from https://rpsychologist.com/cohend/\" width=\"483\"}\n\nThat's a big distortion, and it really matters. It means:\n\n-   What seems like a major/breakthrough effect may actually be more modest\n\n-   Follow-up studies done at the same sample-size are more likely than not to fail!\n\nThis is the difficult truth about inadequate sample-sizes: it's not just about the waste of not detecting true effects. Inadequate samples are problematic because the significant findings they generate are likely to be distorted and misleading. It's very difficult to do fruitful, generative science if what you publish is a distorted view of reality.\n",
    "supporting": [
      "What-Not-To-Do_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}