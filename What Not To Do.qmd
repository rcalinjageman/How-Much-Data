---
title: "How *Not* to Get Your Sample Size"
---

{{< video https://youtu.be/dwyYmGmdFHA >}}

What not to do...

## Exploration: The Perils of Inadequate Samples, Even When *p* \< .05

Let's explore why inadequate samples are so problematic.

Imagine you are measuring the effect of maternal separation on gene expression in the hippocampus. You'll have two groups: Control and Separated. You'll use qPCR to measure expression of *egr1*, a constituitvely-expressed gene that plays an important role in learning and memory.

Without a sample-size plan, you'd likely rely on the previous literature or lab practices to set a sample size. In qPCR experiments like this one, *n* = 6/group is pretty common, so that's what you decide to go with.

Let's suppose that in this case, your research hypothesis is absolutely correct: maternal separation *does* produce a large and robust increase in *egr1* expression. We'll set the effect size at 1 standard deviation (Cohen's *d* = 1; see the section on effect sizes if you're not sure what this means).

This sounds like an ideal situation: the researcher has a hypothesis that is actually correct! Let's see, though, what happens when this true hypothesis is investigated with *n* = 6/group.

In the simulation below, we randomly generate gene-expression data for the Control and Separated groups, drawing the data from distributions that are 1 standard deviation apart. With each draw we then run a t-test and check to see if it is statistically significant or not. Given that the researcher's hypothesis is true, we might expect this experiment to always "work", regularly producing *p* \< .05. In fact, though, it does not.!

```{r}
library(statpsych)
library(ggplot2)
library(gganimate)

effect_size_in_sds <- 1
samples_per_group <- 6
alpha <- 0.05


simcount <- 10000

sigfindings <- 0

all_res <- data.frame(d = NA, p = NA, distortion = NA, sample = NA, CIlength = NA)

for (i in 1:simcount) {
  control <- rnorm(samples_per_group, 0, 1)
  experimental <- rnorm(samples_per_group, 0, 1) + effect_size_in_sds
  myres <- t.test(control, experimental)
  if (myres$p.value < alpha) sigfindings <- sigfindings + 1
  myd <- statpsych::ci.stdmean2(
    alpha = alpha,
    m2 = mean(control),
    m1 = mean(experimental),
    sd2 = sd(control),
    sd1 = sd(experimental),
    n2 = length(control),
    n1 = length(experimental)
  )
  
  all_res[i, ] <- c(
    myd[1, 2],
    myres$p.value,
    myd[1, 2] / effect_size_in_sds,
    i,
    myd[1, 5] - myd[1, 4]
  )
  
}

print(paste("Simulated", simcount, "2-group experiments with n =", samples_per_group, "/group and a true effect of", effect_size_in_sds, "standard deviations."))
print("Proportion of statistically significant findings (p < .05):")
print(sigfindings/simcount)
print("Typical margin of error:")
print(mean(all_res$CIlength/2))
```

What we find, instead is that only about 33% of experiments yield *p* \< .05. Why so low? Well, consider that statistical tests examine if the difference observed (signal) is substantially greater than expected sampling error (noise). With the sample-size we selected, though, expected sampling error is large: in fact, the margin of error in these studies is typically \~1.37 standard deviations, far larger than the signal it would be reasonable to expect!

If the noise in this type of study is bigger than the signal, how do any of the experiments still turn out to be statistically significant? This occurs when sampling error breaks in just the right way to *distort* the effect–to make it seem much bigger than it really is, so that, for that misleading sample at least, the effect observed is substantially larger than the real truth. Uh oh! That's right, in these cases *p* \< .05 can only occur *by mis-characterizing the actual truth*.

Let's see this in more detail. We're going to plot the effect observed in each study. The line at 1 standard deviation represents the true effect. Because of sampling error, studies "dance" around this truth – some get a bit too large of an effect, others a bit too small. Notice, though, the coloring, which represents statistical significance. Most of the dots are blue, not significant – that's the low power of the experiment. But note that the dots that are red are all studies that radically *over-estimated* the real truth.

```{r}
time_plot <- ggplot(data = all_res[all_res$sample < 200, ], aes(x=sample, y = d, colour = (p < alpha)))
time_plot <- time_plot + ylab("Standardized Mean Difference in egr Expression (Experimental vs. Control)")
time_plot <- time_plot + geom_point(aes(group = sample))
time_plot <- time_plot + theme_classic() + theme(legend.position = "none")
time_plot <- time_plot + scale_colour_manual(values = c("TRUE" = "red", "FALSE" = "dodgerblue1"))
time_plot <- time_plot + geom_hline(yintercept = 0, linetype = "dotted")
time_plot <- time_plot + geom_hline(yintercept = 1)
time_plot <- time_plot + transition_reveal(sample)
animate(time_plot, fps = 6, renderer = gifski_renderer("d_over_time_updated.gif"))
```

How bad is this over-estimation? Let's find out:

```{r}
print("True effect size = 1 standard deviation")
print("Across all stimulated studies, the average effect size observed is:")
mean(all_res$d)
print("But for the studies that reached statistical significance, the average effect size observed is:")
mean(all_res[all_res$p < .05, ]$d)

```

Wow! The statistically-significant effects over-state the truth by over 70%.

![True Effect Size](images/rpsychologist-cohend.svg){width="476"}

![Observed Effect Size when p \< .05](images/rpsychologist-cohend(1).svg){width="483"}

That's a big distortion, and it really matters. It means:

-   What seems like a major/breakthrough effect may actually be more modest

-   Follow-up studies done at the same sample-size are more likely than not to fail!

This is the difficult truth about inadequate sample-sizes: it's not just about the waste of not detecting true effects, it's also that they only succeed by distorting the truth and making modest effects seem massive. It's very difficult to do fruitful, generative science if what you publish is a distorted view of reality.
