[
  {
    "objectID": "Why You Need Sample-Size Planning.html",
    "href": "Why You Need Sample-Size Planning.html",
    "title": "Why You Need Sample-Size Planning",
    "section": "",
    "text": "Why is sample-size planning so important? The short video above takes you through some extrinsic reasons (funders demand it!) and some more important intrinsic reasons (it is an ethical obligation and a key to fruitful science!).\nWant to dig deeper? Here are some of the guidelines and reporting requirements mentioned:\n\nNIH emphasis on rigor and reproducibility\n\nAs of 2016, NIH has adopted a new initiative on Rigor and Reproducibility that stress evaluation of project proposals for their ability to produce robust and unbiased results.\nIn explaining this new policy, sample-size planning was listed as a way to help meet this new evaluation criterion. See the NIH blog post here:\n\nReporting Guidelines -\n\nThe ARRIVE guidelines provide very useful advice for reporting in vivo experiments. It includes the requirement of reporting sample sizes clearly and justifying how they were set.\nThe NIH also helped organize a set of principles for the reporting of pre-clinical research; these guidelines were endorsed by a wide variety of journals and professional societies. Here are the NIH guidelines. The guidelines related to transparency stipulates that authors should explain their sample-size determinations.\n\nNature Neuroscience announced updated standards in 2013 editorial and released a reporting checklist authors should complete on submission that requires sample-size planning.\nJournal of Neuroscience has issued updated author guidelines as of March of 2017 that asks for sample-size justification.\n\n\nEthical considerations - the American Stasitical Association has put forth ethical guidelines for those who regularly use statistics. These enjoin statisticians to collect neither too much nor too little data (as both are ethically problematic). The guidelines are online here.",
    "crumbs": [
      "Course",
      "Why You Need Sample-Size Planning"
    ]
  },
  {
    "objectID": "webexercises.html",
    "href": "webexercises.html",
    "title": "Webexercises",
    "section": "",
    "text": "This is a Web Exercise template created by the psychology teaching team at the University of Glasgow, based on ideas from Software Carpentry. This template shows how instructors can easily create interactive web documents that students can use in self-guided learning.\nThe {webexercises} package provides a number of functions that you use in inline R code or through code chunk options to create HTML widgets (text boxes, pull down menus, buttons that reveal hidden content). Examples are given below. Render this file to HTML to see how it works.\nNOTE: To use the widgets in the compiled HTML file, you need to have a JavaScript-enabled browser."
  },
  {
    "objectID": "webexercises.html#example-questions",
    "href": "webexercises.html#example-questions",
    "title": "Webexercises",
    "section": "Example Questions",
    "text": "Example Questions\n\nFill-In-The-Blanks (fitb())\nCreate fill-in-the-blank questions using fitb(), providing the answer as the first argument.\n\n2 + 2 is \n\nYou can also create these questions dynamically, using variables from your R session.\n\nThe square root of 64 is: \n\nThe blanks are case-sensitive; if you don’t care about case, use the argument ignore_case = TRUE.\n\nWhat is the letter after D? \n\nIf you want to ignore differences in whitespace use, use the argument ignore_ws = TRUE (which is the default) and include spaces in your answer anywhere they could be acceptable.\n\nHow do you load the tidyverse package? \n\nYou can set more than one possible correct answer by setting the answers as a vector.\n\nType a vowel: \n\nYou can use regular expressions to test answers against more complex rules.\n\nType any 3 letters: \n\n\n\nMultiple Choice (mcq())\n\n“Never gonna give you up, never gonna: let you goturn you downrun awaylet you down”\n“I bless the rainsguess it rainssense the rain down in Africa” -Toto\n\n\n\nTrue or False (torf())\n\nTrue or False? You can permute values in a vector using sample(). TRUEFALSE\n\n\n\nLonger MCQs (longmcq())\nWhen your answers are very long, sometimes a drop-down select box gets formatted oddly. You can use longmcq() to deal with this. Since the answers are long, It’s probably best to set up the options inside an R chunk with echo=FALSE.\nWhat is a p-value?\n\n the probability that the null hypothesis is true the probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is true the probability of making an error in your conclusion\n\nWhat is true about a 95% confidence interval of the mean?\n\n 95% of the data fall within this range if you repeated the process many times, 95% of intervals calculated in this way contain the true mean there is a 95% probability that the true mean lies within this range"
  },
  {
    "objectID": "webexercises.html#checked-sections",
    "href": "webexercises.html#checked-sections",
    "title": "Webexercises",
    "section": "Checked sections",
    "text": "Checked sections\nCreate sections with the class webex-check to add a button that hides feedback until it is pressed. Add the class webex-box to draw a box around the section (or use your own styles).\n\nI am going to learn a lot: TRUEFALSE\nWhat is a p-value?\n\n the probability that the null hypothesis is true the probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is true the probability of making an error in your conclusion"
  },
  {
    "objectID": "webexercises.html#hidden-solutions-and-hints",
    "href": "webexercises.html#hidden-solutions-and-hints",
    "title": "Webexercises",
    "section": "Hidden solutions and hints",
    "text": "Hidden solutions and hints\nYou can fence off a solution area that will be hidden behind a button using hide() before the solution and unhide() after, each as inline R code. Pass the text you want to appear on the button to the hide() function.\nIf the solution is a code chunk, instead of using hide() and unhide(), simply set the webex.hide chunk option to TRUE, or set it to the string you wish to display on the button.\nRecreate the scatterplot below, using the built-in cars dataset.\n\n\n\n\n\n\n\n\n\n\n\nI need a hint\n\nSee the documentation for plot() (?plot)\n\n\n\n\n\nClick here to see the solution\n\nplot(cars$speed, cars$dist)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To How Much Data",
    "section": "",
    "text": "This is the first post to the How Much Data blog! More to come!"
  },
  {
    "objectID": "posts/confirmation-bias-paper/confirmation-bias-paper.html",
    "href": "posts/confirmation-bias-paper/confirmation-bias-paper.html",
    "title": "Stop Fooling Yourself - paper by Richard Born",
    "section": "",
    "text": "Just as important as a reasonable sample is good study design that mitigates the risks of bias.\nTo that end, eNeuro has a great new paper by Richard Born (Harvard) about how to diagnose and avoid confirmation bias (Born 2024). The full text is here: https://www.eneuro.org/content/11/10/ENEURO.0415-24.2024\nThis paper is part of a series on Improving Your Neuroscience that I (R.C-J.) am helping to organize (Calin-Jageman 2024). You can find a list of all papers as they appear in this series here: https://www.eneuro.org/collection/improving-your-neuroscience\nI strongly recommend this lovely paper – it is full of fascinating examples and references; it is the type of paper you’ll want to assign to all your incoming trainees.\n\n\n\nStop Fooling Yourself paper by Richard T. Born\n\n\n\n\n\n\nReferences\n\nBorn, Richard T. 2024. “Stop Fooling Yourself! (Diagnosing and Treating Confirmation Bias).” eNeuro 11 (10). https://doi.org/10.1523/ENEURO.0415-24.2024.\n\n\nCalin-Jageman, Robert J. 2024. “New eNeuro Series: Improving Your Neuroscience.” eNeuro 11 (3). https://doi.org/10.1523/ENEURO.0048-24.2024."
  },
  {
    "objectID": "Planning for Power.html",
    "href": "Planning for Power.html",
    "title": "Planning for Power",
    "section": "",
    "text": "The video above gives a quick overview of planning for power. Once you’ve viewed it, this page helps you dig into some details.",
    "crumbs": [
      "Course",
      "Three Approaches",
      "Planning for Power"
    ]
  },
  {
    "objectID": "Planning for Power.html#the-inputs-you-need-for-planning-for-power",
    "href": "Planning for Power.html#the-inputs-you-need-for-planning-for-power",
    "title": "Planning for Power",
    "section": "The Inputs You Need for Planning for Power",
    "text": "The Inputs You Need for Planning for Power\nSuppose you are in a lab that studies neurogenesis. You believe that early-life stress impairs neurogenesis. You will test your theory by exposing lab mice to stress or to standard living conditions. You will then use BRDU to label new neurons in the hippocampus. How can you make a good sampling plan for your study? (This example is inspired by (Mirescu, Peters, and Gould 2004)).\nTo start, you’ll need some inputs. Specifically, you need to know:\n\nThe stringency of your hypothesis test you will conduct (alpha = .05 is typical)\nThe level of power you want (80%? 90%? 95%?). The lower the power the lower your sample-size need. But the lower the power, the higher your chance of missing an effect if it is there. Just as bad, low-power samples that yield significant results often do so by inflating the true effect (see What Not to Do)\nYou also need to know the exact type of test you want to use. Will you use a t-test to compare means with the assumption of equal variance? Or, do you want to use a t-test to compare means without assuming equal variance (often called a Welch’s t-test)? Or, because you are dealing with count data, do you want to compare medians using a non-parametric test (e.g. the Mann-Whitny U test).\nYou also need a predicted effect: By how much do you believe stress will impact neurogenesis?\n\nIdeally you would have a prediction in raw units. Meaning, for example, you have a good sense of the typical level of neurogenesis under control conditions and you have a prediction of how much less neurogenesis you expect with stress (500 less neurons? 700 less neurons?). Ideally, this prediction is directional (predicting either a decrease or an increase over controls). You can, however, make a non-directional prediction (a change of at least 500 neurons)… though if you don’t know the direction, it’s hard to imagine the prediction being very well-founded.\n\nIf you go this route (which is best), you also need to provide a good estimate of the standard deviation in both groups.\n\nYou could also make your prediction in standard deviation units (1 standard deviation reduction, 2 standard deviation reduction, etc.).\n\nIf you go this route, you don’t need a predicted standard deviation for each group. That sounds attractive – you can get by with less guesswork. But on the other hand, what’s the basis of your prediction? If it is a well-founded prediction, it would usually come from familiarity with the assay and some knowledge of both typical scores and their variety. If you don’t know anything about the variety to expect in your assay, it’s hard to imagine your prediction in standard deviation units will be that informed.\n\n\n\nWhen asked for these inputs, many researchers become frustrated: How am I supposed to know all this before I do the study!? It’s true the inputs required for planning for power are extensive. But that’s because we plan for power to conduct a hypothesis test, and a hypothesis test is meant to test a hypothesis. If you don’t have a clear, quantitative hypothesis, then you aren’t really ready to conduct a hypothesis test, and if you’re not ready for a hypothesis test, you will definitely have difficulty planing one!\nSo: if you find the inputs for planning for power daunting you may want to consider: Am I ready to conduct a hypothesis test? Perhaps you want to start with some descriptive research to accurately characterize the system you are studying. In that case, you might want to plan for precision (for accurate description), and then, once you’ve described the system well you might formulate clear hypotheses that are worth testing.\nThis warning that you may not be ready for a hypothesis test may sound absurd. Just look at any journal and you’ll see hypothesis test after hypothesis test… clearly those researchers didn’t always have clear quantitative hypotheses and all this background knowledge. True – current norms are to use hypothesis tests willy nilly, in circumstances where the researcher has no clear hypotheses and little forethought. The regularity with which hypothesis testing is abused, however, does not make it wise, sensible, or sound. You can find more details on why hypothesis testing should be reserved for testing hypotheses in these sources (Calin-Jageman 2022; Scheel et al. 2020).",
    "crumbs": [
      "Course",
      "Three Approaches",
      "Planning for Power"
    ]
  },
  {
    "objectID": "Planning for Power.html#planning-for-power-with-statpsych",
    "href": "Planning for Power.html#planning-for-power-with-statpsych",
    "title": "Planning for Power",
    "section": "Planning for Power with statpsych",
    "text": "Planning for Power with statpsych\nWhat if you really are ready for a hypothesis test? Well there are tons of tools you can use to help you plan for power. In this workshop we’ll demonstrate the use of statpsych, a package for R by Doug Bonett. Why? Because statpsych is easy to use, comprehensive, has a consistent and well-documented set of functions, and has been extensively validated.\nLet’s continue our neurogenesis example. Suppose from previous research you do know a good bit about neurogenesis under control conditions: your lab has typically found about 3,500 neurons labelled within 2 hours of BRDU injection, with a standard deviation of 500. Furthermore, from previous research on neurogenesis you believe that stress will alter neurogenesis by a substantial amount, at least 700 neurons –that’s more than 1 standard deviation. Note that, for now we said “a change of at least”… that’s a non-directional hypothesis; we’ll start with this vague hypothesis and then switch to a directional hypothesis in a moment.\nWe need two additional inputs: stringency (alpha) and desired power. We’ll pick a traditional stringency (.05) and we’ll shoot for 90% power. You’ll often see 80% used as a convention, but who wants to let 20% of their true hypotheses fail to garner support? If the experiment will require considerable time and resources and is worth doing, then it is probably worth doing with a relatively high rate of power.)\nWe can put all this together and find our needed sample-size using the size.test.mean2 function in statpsych. This function, along with all other functions in statpsych, is documented here: https://dgbonett.github.io/statpsych/reference/index.html\n\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\nLoading required package: statpsych\n\n# Get sample size for 90% power for a two-group hypothesis test of at least 700-neuron change \n#  with an assay with typical standard deviaiton of 500 neurons.  The test will be conducted\n#  with an alpha of .05.  We'll plan for equal sample-sizes in both groups.\nstatpsych::size.test.mean2(\n  alpha = 0.05,\n  pow = 0.90,\n  var = 500^2,     # Typical sd in our lab is 500 neurons; here we input variance (sd^2)\n  es = 700,\n  R = 1            # ratio of n1/n2; here we entered 1 for equal group sizes\n)\n\n n1 n2\n 12 12\n\n\nAccording to these calculations, we need n = 12 per group (24 animals in total) to conduct a well-powered test of our hypothesis. This is a good bit more than is typically used in neurogenesis research… but that’s a reflection of poor prior practice, not bad planning on our part.\nIf you are really ready for a hypothesis test then you almost certainly have a directional hypothesis. That’s a good thing, because that will reduce our sample-size needs relative to a non-direcitonal hypothesis. There is a cost, though: by making a directional hypothesis you are calling your shot. That is, you are committing to conducting the test only if the effect is in the predicted direction – otherwise it is a non-significant finding even if it would have been significant in the other direction. By making that commitment, you can get your desired power with less resources. Clearly, though, it would be best to document that commitment; a public pre-registration would be ideal. Unfortunately, abuse of directional tests has led to suspicion that they reflect p hacking–that’s a shame because if we used hypothesis testing for testing actual hypotheses we would almost always be conducting direcitonal effects–it is actually non-directional hypotheses that clue us to the fact that the researcher doesn’t seem to have any clear idea of what they’re predicing.\nAnyway, how do we plan for a directional hypothesis? Some tools have this as a specific option, but in general what you do is double the alpha used in your input. That is, for a .05 directional test, you can use .10 in a non-directional tool. Here’s what we’d get with statpsych:\n\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\n# Get sample size for 90% power for a two-group hypothesis test of at least 700-neuron change \n#  with an assay with typical standard deviaiton of 500 neurons.  The test will be conducted\n#  with an alpha of .05.  We'll plan for equal sample-sizes in both groups.\nstatpsych::size.test.mean2(\n  alpha = 0.05 * 2,  # Double alpha for directional test\n  pow = 0.90,\n  var = 500^2,     # Typical sd in our lab is 500 neurons; here we input variance (sd^2)\n  es = 700,\n  R = 1            # ratio of n1/n2; here we entered 1 for equal group sizes\n)\n\n n1 n2\n 10 10\n\n\nNice! With a directional effect we can get by with n = 10/group or 20 animals overall. That’s a resource saving of 20%!",
    "crumbs": [
      "Course",
      "Three Approaches",
      "Planning for Power"
    ]
  },
  {
    "objectID": "Planning for Power.html#important-considerations-when-planning-for-power",
    "href": "Planning for Power.html#important-considerations-when-planning-for-power",
    "title": "Planning for Power",
    "section": "Important Considerations When Planning for Power",
    "text": "Important Considerations When Planning for Power\n\nWhat if your inputs are off? That, is what if variation is 20% higher than you expected? What if the effect is 20% weaker than you expected? Or 50%? The best practice is to explore a variety of inputs and judiciously choose a sample size. Sample-size needs can be much larger than you might be used to. Be sure to check out the section on optimization for some tips on how to deal with this.\nDoes your sample-size plan matches your research question? It sounds obvious, but you need to make sure you are planning for power for the analysis that answers your research quesiton. Note that with hypothesis tests that can sometimes be confusing. For example, suppose you test the effects of both stress (high/low) and sex (male/female) on neurogensis. You will probably want to test if stress affects neurogenesis in male mice. And you will probably want ot test if stress affects neurogenesis in female mice. But most importantly, you will probably want to know if there is a sex difference in the impact of stress (an interaction). In that case, you need to make sure your sample-size plan is for this interaction, which typically has considerably higher sample-size needs than either simple effect.\nWill you conduct multiple tests? When you will conduct multiple tests your risk of including a false-positive. You thus need to make a sample size plan that includes the increase needs for when adjusting for multiple comparisons. This can become complicated, and sometimes can only be solved through simulation. On the other hand, if you are using hypothesis testing only to test well-formed, quantitative hypotheses you may find yourself with only a few focal hypotheses to test in any given study.\nDo you have non-independent/hierarchical data? Hierarchical data (e.g. multiple cells recorded from the each animal in each condition) violates the non-independence assumed in most statistical tests. You will need to make sure you have a porper analysis strategy (e.g. sufficient summary statistics approach) and a power plan that matches.",
    "crumbs": [
      "Course",
      "Three Approaches",
      "Planning for Power"
    ]
  },
  {
    "objectID": "Planning for Power.html#planning-for-power-for-other-designs",
    "href": "Planning for Power.html#planning-for-power-for-other-designs",
    "title": "Planning for Power",
    "section": "Planning for Power for Other Designs",
    "text": "Planning for Power for Other Designs\nstatpsych has a wide range of functions for planning for power for simple designs (https://dgbonett.github.io/statpsych/reference/index.html). These include:\n\n\n\nsize.test.cor()\n\nSample size for a test of a Pearson or partial correlation\n\n\n\nsize.test.cor2()\n\nSample size for a test of equal Pearson or partial correlation in a 2-group design\n\n\n\nsize.test.lc.ancova()\n\nSample size for a mean linear contrast test in an ANCOVA\n\n\n\nsize.test.lc.mean.bs()\n\nSample size for a test of a between-subjects mean linear contrast\n\n\n\nsize.test.lc.mean.ws()\n\nSample size for a test of a within-subjects mean linear contrast\n\n\n\nsize.test.lc.prop.bs()\n\nSample size for a test of between-subjects proportion linear contrast\n\n\n\nsize.test.mann()\n\nSample size for a Mann-Whitney test\n\n\n\nsize.test.mean.ps()\n\nSample size for a test of a paired-samples mean difference\n\n\n\nsize.test.mean()\n\nSample size for a test of a mean\n\n\n\nsize.test.mean2()\n\nSample size for a test of a 2-group mean difference\n\n\n\nsize.test.prop.ps()\n\nSample size for a test of a paired-samples proportion difference\n\n\n\nsize.test.prop()\n\nSample size for a test of a single proportion\n\n\n\nsize.test.prop2()\n\nSample size for a test of a 2-group proportion difference\n\n\n\n\nOne thing you’ll notice that’s missing is interactions for complex designs – these are currently not available in statpsych. There are a number of good tools for more complex designs; one of http://www.intxpower.com/ and this companion paper (Sommet et al. 2023).",
    "crumbs": [
      "Course",
      "Three Approaches",
      "Planning for Power"
    ]
  },
  {
    "objectID": "News.html",
    "href": "News.html",
    "title": "News",
    "section": "",
    "text": "Changelog\n\n2024-10-31 - Launched!\n\nRoadmap\n\nProvide more model sample-size plans\nProvide a flow-chart and discussion of how to choose between planning for power and planning for precision\nAdd more extensive and detailed exercises"
  },
  {
    "objectID": "LICENSE-CC-BY-NC-4.0.html",
    "href": "LICENSE-CC-BY-NC-4.0.html",
    "title": "Attribution-NonCommercial 4.0 International",
    "section": "",
    "text": "The How Much Data website, videos, and resources are provided through a Creative Commons Attribution-NonCommercial License (see below). You may share (mirror) and adapt (borrow and alter) any/all course content, provided you credit the source and that you do not use these materials or your adaptations for commercial purposes."
  },
  {
    "objectID": "LICENSE-CC-BY-NC-4.0.html#creative-commons-attribution-noncommercial-4.0-international-public-license",
    "href": "LICENSE-CC-BY-NC-4.0.html#creative-commons-attribution-noncommercial-4.0-international-public-license",
    "title": "Attribution-NonCommercial 4.0 International",
    "section": "Creative Commons Attribution-NonCommercial 4.0 International Public License",
    "text": "Creative Commons Attribution-NonCommercial 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nNonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\nB. produce, reproduce, and Share Adapted Material for NonCommercial purposes only.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\nIf You Share Adapted Material You produce, the Adapter’s License You apply must not prevent recipients of the Adapted Material from complying with this Public License.\n\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to How Much Data?",
    "section": "",
    "text": "Welcome to How Much Data?, an online workshop on sample-size determination.\nHere you will find:\n\nA self-paced course to help you gain skills with sample-size planning\nA list of resources for helping you on your sample-size planning journey\nA blog with updates and news relevant for sample-size planning\nThe source code for this course and website that you may use to help customize these resources for your own training purposes and programs.\n\nThis course was developed by Bob Calin-Jageman at Dominican University. Your feedback, comments, and bug reports are very welcome – submit them via Github here.\nThis course has been a long time in the making and is still under active development. Expect addition of new topics (e.g. Simulation for Sample-Size Planning), additional simulations, and more.\nThe development of this course was supported by the NIGMS division of the National Institute of Health through grant 5R25GM132784-02. You can find other training modules focused on Rigor and Reproducibility here: https://www.nigms.nih.gov/training/pages/clearinghouse-for-training-modules-to-enhance-data-reproducibility.aspx"
  },
  {
    "objectID": "Decreasing Sample-Size Needs.html",
    "href": "Decreasing Sample-Size Needs.html",
    "title": "Decreasing Sample-Size Needs",
    "section": "",
    "text": "When you get serious about sample-size planning, you can often find that it takes much larger samples than you’d like to get clear answers to your scientific questions. While there is no doubt that many fields have settled on demonstrably inadequate sample sizes, the answer doesn’t have to be solely focused on collecting more data: we can also design better studies!\nThe video above gives an example of optimization - the art of coaxing more out of your data. There are three basic approaches:",
    "crumbs": [
      "Course",
      "Decreasing Sample-Size Needs"
    ]
  },
  {
    "objectID": "Decreasing Sample-Size Needs.html#reducing-variation-is-gold",
    "href": "Decreasing Sample-Size Needs.html#reducing-variation-is-gold",
    "title": "Decreasing Sample-Size Needs",
    "section": "Reducing Variation is Gold",
    "text": "Reducing Variation is Gold\nImagine you are characterizing neurogenesis in animals raised in stressed vs. standard conditions. From prevous studies, you know expect control animals to have about 3500 new neurons labelled 1 hour after BRDU injection, with a standard deviation of 500. You expect stress to have a pretty notable effect, say a 20% reduction to 2,800 labelled neurons, a difference of 3,500 - 2,800 = 700 neurons. This example is inspired by (Mirescu, Peters, and Gould 2004) (note that we’ll analyze this scenario with t-tests, as the original authors did, even though count data is typically not normally distributed and therefore not suitable for analysis in this way).\nLet’s say the typical study in your field uses n = 6/group. Is that adequate? Let’s find out with statpsych:\nWhat’s alpha = 0.05, what sample size do you need for 90% power? Let’s find out with statpsych:\n\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\nLoading required package: statpsych\n\n# Estimate power for a 2-group design\nstatpsych::power.mean2(\n  alpha = 0.05,\n  n1 = 6,\n  n2 = 6,\n  var1 = 500^2,\n  var2 = 500^2,\n  es = 3500-2800\n)\n\n     Power\n 0.5906058\n\n\nUh oh, power is only 60%. That means we have a high risk of missing true effects and that statistically significant effects are likely to be inflated (see What Not To Do).\nWe could throw more data at the problem. But first, what would happen if we could reduce our sampling variability? Imagine, for example, that we might better-standardize our cell counts, perhaps by having two trainees count them independently and use the average. We could also refine what to do with borderline cases, and maybe standardize our injection and dissection protocols a little better. Suppose through these steps we could reduce within-group variation by just 20%. What would that do to our power?\n\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\n# Same scenario, within group sd reduced by 20% through optimization\nstatpsych::power.mean2(\n  alpha = 0.05,\n  n1 = 6,\n  n2 = 6,\n  var1 = (500 * .8) ^2,   # Imagine reducing the sd to 80% of what your lab typically obtains\n  var2 = (500 * .8) ^2,   # same reduction in both groups\n  es = 3500-2800\n)\n\n     Power\n 0.7797131\n\n\nThat’s a big jump in power! We’re now getting close to a reasonable power with the same number of samples.\n\nWould a 20% increase in sample size have the same impact?\n\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\n# Same scenario but increase sample-size by 20%... not the same impact!\nstatpsych::power.mean2(\n  alpha = 0.05,\n  n1 = 6 * 1.2,\n  n2 = 6 * 1.2,\n  var1 = 500^2,\n  var2 = 500^2,\n  es = 3500-2800\n)\n\n     Power\n 0.6868732\n\n\nNo! Why not? Well, recall that formula for the standard error of the mean:\n\\[\nsigma_{M} = frac{sigma}{sqrt(N)}\n\\]\nThis shows us that changes in within-group variation is directly related to expected sampling error, whiles sample-size is only related by its square root. That means that reducing noise (when possible) can be much more impactful than increasing sample size. If takes a 56% increase in sample-size to obtain the same benefit of a 20% reduction in within-group standard deviation!\n\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\n# To get the same impact through sample-size, we need ~50% increase\nstatpsych::power.mean2(\n  alpha = 0.05,\n  n1 = 6 * 1.5,\n  n2 = 6 * 1.5,\n  var1 = 500^2,\n  var2 = 500^2,\n  es = 3500-2800\n)\n\n   Power\n 0.79613",
    "crumbs": [
      "Course",
      "Decreasing Sample-Size Needs"
    ]
  },
  {
    "objectID": "Decreasing Sample-Size Needs.html#more-gold-optimizing-for-larger-effects",
    "href": "Decreasing Sample-Size Needs.html#more-gold-optimizing-for-larger-effects",
    "title": "Decreasing Sample-Size Needs",
    "section": "More Gold: Optimizing for Larger Effects",
    "text": "More Gold: Optimizing for Larger Effects\nIn addition to reducing noise, we can work on maximizing signal. We might extend our treatement (longer stress), increase the magnitude of treatment (stronger stress), and/or focus in on measures which are especially susceptible to the treatment. For example, we might find that only some layers of the hippocampus undergo significant neurogenesis. If we could restrict our labelling to these layers, we could avoid having our effect diluted by unaffected measures.\nAs with reducing noise, increasing signal gets us a lot more bang for the buck. Let’s continue the previous example (2-group design, 6 per group, reduction of neurogenesis by 700 neurons, within-group standard deviaiton of 500 neurons).\nAgain, here is our ‘standard scenario’, in which we learn we have an inadequate sample size:\n\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\n# Estimate power for a 2-group design\nstatpsych::power.mean2(\n  alpha = 0.05,\n  n1 = 6,\n  n2 = 6,\n  var1 = 500^2,\n  var2 = 500^2,\n  es = 3500-2800\n)\n\n     Power\n 0.5906058\n\n\nAnd now let’s check our power if we can increase the effect size by just 20%\n\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\n# Estimate power for a 2-group design\nstatpsych::power.mean2(\n  alpha = 0.05,\n  n1 = 6,\n  n2 = 6,\n  var1 = 500^2,\n  var2 = 500^2,\n  es = (3500-2800) * 1.2   # increase effect size\n)\n\n     Power\n 0.7463204\n\n\nWow! We’ve get to nearly reasonable power without needing more resources.\nCertainly there are limits to what optimization can do, but working diligently to increase signal and decrease noise can help you get clearer answers with the same resources! That’s the type of thing that can make a huge difference over the course of your career.",
    "crumbs": [
      "Course",
      "Decreasing Sample-Size Needs"
    ]
  },
  {
    "objectID": "Decreasing Sample-Size Needs.html#design-matters-too",
    "href": "Decreasing Sample-Size Needs.html#design-matters-too",
    "title": "Decreasing Sample-Size Needs",
    "section": "Design Matters, Too",
    "text": "Design Matters, Too\nOur experimental design can also influence the efficiency of our experiment. In general, the simple two-group design is the least efficient design. Within-subjects designs typically have much more bang for the buck.\nLet’s take a look at the benefits, this time using an precision approach. Here we’ll use statpsych to simulate converting a between-subjects study to within-subjects. For each set of simulations we’ll focus on the typical confidence interval width.\nFirst, the between-subjects scenario. We’ll again work with 6 animals per group. We’ll assume the groups have equal variation (sd.ratio = 1) and that both come from normal distributions (dist1 = 1; dist2 = 1; where 1 tells statpsych to simulate draws from a normal distribution). We’ll conduct 1000 studies and report the average 95% confidence-interval width in standard deviation units:\n\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\n# Base scenario - 6 animals/group, between subjects, equal variance\nstatpsych::sim.ci.mean2(\n  alpha = 0.05,\n  n1 = 6,\n  n2 = 6,\n  sd.ratio = 1,\n  dist1 = 1,\n  dist2 = 1,\n  rep = 1000\n)\n\n                             Coverage Lower Error Upper Error Ave CI Width\nEqual Variances Assumed:        0.948       0.029       0.023     2.503216\nEqual Variances Not Assumed:    0.951       0.029       0.020     2.556332\n\n\nWow! Our typical confidence interval will be ~2.5 standard deviations wide! That’s a lot of uncertainty, showing that our sample-size is appropriate only for assays in which we are justified in expecting truly massive effects.\nWhat if we ran the same study as a within-subject design? We’ll keep everything the same, but will also specifiy the correlation between pre/post measures. We’ll use 0.70, which is a reasonable estimate for a measure that has reasonable reliability.\nWhile within-subjects designs may not be feasible for studies in which the measurement requires destruction of the sample, matched-control designs can offer some of the same benefits.\n\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\n# Switch to within-subjects, just n = 6, correlation of .7 between repeated measures\nstatpsych::sim.ci.mean.ps(\n  alpha = 0.05,\n  n = 6,\n  sd.ratio = 1,\n  cor = 0.7,\n  dist1 = 1,\n  dist2 = 1,\n  rep = 1000\n)\n\n Coverage Lower Error Upper Error Ave CI Width\n     0.95       0.026       0.024     1.549645\n\n\nHoly cow! We are now using 1/2 the animals (1 group of 6 rather than 2 groups of 6), but our confidence interval is now much reduced, to about 1.5 standard deviations in length. That’s still quite long, and only acceptable for assays where we expect pretty large effects… but much despite using 1/2 the resources. And what if we kept with 12 animals, but all in the within-subjects design?\n\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\n# Switch to within-subjects, n = 12\nstatpsych::sim.ci.mean.ps(\n  alpha = 0.05,\n  n = 12,\n  sd.ratio = 1,\n  cor = 0.7,\n  dist1 = 1,\n  dist2 = 1,\n  rep = 1000\n)\n\n Coverage Lower Error Upper Error Ave CI Width\n    0.951       0.027       0.022    0.9700421\n\n\nNice - we’ve got our precision down to ~1 standard deviation without increasing our sample-size. Of course, we need to think critically about if a control/untreated design is needed – but we might be able to show no effect in control once and leverage that finding for repeated mechanistic studies with within-subjects designs–getting a lot more out of each experiment without much more in resources!\nFor many studies, within-subjects measurement is simply not feasible. With neurogenesis, for example, counting new neurons requires sacrificing the animal subjects, so additional measures are no longer feasible. In those cases, though, matched control designs and/or rigorous selection of covariates can provide many of the same benefits. For example, we could conduct a matched-control design by pre-testing stress-reactivity in all animals prior to the stress manipulation, making matched pairs of similar reactivity, and conducting random assignment to treatment within each pair. Depending on how linked scores are across match pairs, we can obtain many of the same benefits of a within-subjects design.",
    "crumbs": [
      "Course",
      "Decreasing Sample-Size Needs"
    ]
  },
  {
    "objectID": "Decreasing Sample-Size Needs.html#further-resources",
    "href": "Decreasing Sample-Size Needs.html#further-resources",
    "title": "Decreasing Sample-Size Needs",
    "section": "Further Resources",
    "text": "Further Resources\nIt’s surprisingly difficult to find good practical advice on optimization. Here are some papers I’ve found helpful:\n\nWritten for marking and consumer researchers, this paper nevertheless has vital tips for maximizing effect size (Meyvis and Van Osselaer 2018).\nFocused on animal research, this paper explains the 3R principles and how you can maximize the information you gain from each study (Lazic 2018).\nHere’s a primer focused on clinical trials, but again with lots of good advice that is broadly applicable (Kraemer 1991)\nA blog post from the OSF with good advice (MacKinnon 2013)\nA broader focus on reducing waste in research (Ioannidis et al. 2014)\nAnd finally, a blog post from Andrew Gelman that has some great advice (“Here Are Some Ways of Making Your Study Replicable. (No, the First Steps Are Not Preregistration or Increasing the Sample Size!),” n.d.)",
    "crumbs": [
      "Course",
      "Decreasing Sample-Size Needs"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "myblog",
    "section": "",
    "text": "Stop Fooling Yourself - paper by Richard Born\n\n\n\n\n\n\nnews\n\n\npapers\n\n\n\n\n\n\n\n\n\nOct 29, 2024\n\n\nBob Calin-Jageman\n\n\n\n\n\n\n\n\n\n\n\n\nstatpsych for Sample-Size Planning\n\n\n\n\n\n\nnews\n\n\ntools\n\n\n\n\n\n\n\n\n\nOct 20, 2024\n\n\nBob Calin-Jageman\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To How Much Data\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\nBob Calin-Jageman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Effect Sizes.html",
    "href": "Effect Sizes.html",
    "title": "Effect Sizes",
    "section": "",
    "text": "An effect size is a quantitative answer to your research question.",
    "crumbs": [
      "Course",
      "Effect Sizes"
    ]
  },
  {
    "objectID": "Effect Sizes.html#effect-sizes-can-be-simple-or-complex",
    "href": "Effect Sizes.html#effect-sizes-can-be-simple-or-complex",
    "title": "Effect Sizes",
    "section": "Effect Sizes Can Be Simple or Complex",
    "text": "Effect Sizes Can Be Simple or Complex\n\nIf you want to know the degree of neurogenesis in the hippocampus of the adult mouse, then the mean number of new neurons (M) observed would be an effect size for your study. Alternatively, you might choose the median number of neurons (Mdn).\nIf you wanted to know how much stress impacts nuerogenesis, then your effect size of interest would likely be the mean difference in neurogenesis between control and stressed mice: Mdiff =Mstressed - Mcontrol. This effect-size is a contrast, representing the difference between groups. While mean differences are typical, you might also consider the median difference: Mdndiff =Mdnstressed - Mdncontrol.\nIf you wanted to know how much sex influences the effect of stress on neurogenesis, then your effect size of interest would likely be the mean difference on the difference. That is, you’d be interested in the simple effect of stress on males, the simple effect of stress on females, and the difference between those differences:\n\nMStress In Males =Mmales_stressed - Mmales_control . This is the simple effect for males\nMStress In Females =Mfemales_stressed - Mfemales_control . This is the simple effect for females\nMinteraction aka M⍙⍙ =MStress In Females - MStress In Males. This is the difference between these simple effects (how different the stress effect is in females relative to males). This is a quantitative expression of a 2x2 interaction. You’ll sometimes see the symbol delta-delta or the phrase “difference on the difference” because this effect size represents the difference between two effects (the difference between two differences).\n\n\nAgain, you could be focused on means, but you can also use medians as your measure of what’s typical, finding the difference: Mdninteraction aka Mdn⍙⍙ =MdnStress In Females - MdnStress In Males",
    "crumbs": [
      "Course",
      "Effect Sizes"
    ]
  },
  {
    "objectID": "Effect Sizes.html#categorical-outcomes-have-effect-sizes-too",
    "href": "Effect Sizes.html#categorical-outcomes-have-effect-sizes-too",
    "title": "Effect Sizes",
    "section": "Categorical Outcomes Have Effect Sizes, Too",
    "text": "Categorical Outcomes Have Effect Sizes, Too\nWe’ve focused here on an experiment with a quantitative outcome (number of neurons), but we may instead have a categorical outcome (e.g. depressed or not). In this case, our effect size is typically a proportion (Pdepressed = NDepressed / NTotal). From there, we have the same progression from simple to more and more complex contrasts:\n\nDepression in a single group, say a random-sample of women living in the USA: PWomen_Depressed NWomen_Depressed / NWomen_Total\nA simple difference or contrast, say the difference in rates of depression for women who have received an intervention vs. controls: Pdiff =PWomen_Depressed_Treated - PWomen_Depressed_Control. This would express the difference in depression rates in the treated group relative to the control group.\nA complex contrast, say the degree to which men and women differ in response to a treatment for depression:\n\nPTreatment_Effect_Women =PWomen_Depressed_Treated - PWomen_Depressed_Control.. This is the simple effect for females\nPTreatment_Effect_Men =PMen_Depressed_Treated - PMen_Depressed_Control.. This is the simple effect for males\nMinteraction aka M⍙⍙ =MTreatment_Effect_Women - MTreatment_Effect_Men. Again, this is a quantitative expression of an interaction, representing the difference in response rate in women relative to controls.",
    "crumbs": [
      "Course",
      "Effect Sizes"
    ]
  },
  {
    "objectID": "Effect Sizes.html#there-are-lots-of-other-simple-and-complex-effect-sizes",
    "href": "Effect Sizes.html#there-are-lots-of-other-simple-and-complex-effect-sizes",
    "title": "Effect Sizes",
    "section": "There are Lots of Other Simple and Complex Effect Sizes",
    "text": "There are Lots of Other Simple and Complex Effect Sizes\nAn effect sizes is the quantitative answer to a research question. Given the diversity of research questions, it should come as no surprise that there are lots of other effect sizes.\nPearson’s r is an effect size – it answer the question: to what degree are these variables linearly related to each other? And, again, you can have a simple effect size (How much are hippocampal volume and spatial reasoning related) and contrasts that involved differences in effect sizes (How much do men and women differ in the relationship between hippocampal volume and spatial reasoning – this question is about the difference in R values between these groups).\nVariance is also an effect size! You might be interested, for example, in diversity in brain volume– in which case the effect size of interest might be the variance in brain volume observed across a set of scans. And, again, simple effects often lead to questions comparing effects (To what extent is diversity in brain size altered in autism? This would be a question about the difference in variance observed between a population with autism and normal controls).\nWe could go on, but hopefully you get the point: although scientists most typically examine differences in means and medians, there are lots of different effect sizes.\nOne thing you may have noticed is that “effect size” is a poor turn of phrase – it describes a quantitative outcome of any study, whether it be observational or experimental. If we compare ventricule size in schizophrenics and controls, we are conducting an observational study, and yet we still call the difference observed an effect size.",
    "crumbs": [
      "Course",
      "Effect Sizes"
    ]
  },
  {
    "objectID": "Effect Sizes.html#interpreting-confidence-intervals",
    "href": "Effect Sizes.html#interpreting-confidence-intervals",
    "title": "Effect Sizes",
    "section": "Interpreting Confidence Intervals",
    "text": "Interpreting Confidence Intervals\nWithin a frequentist approach, there are some choices. For a given effect size we could report its standard error (typical expected error), margin of error (largest error expected in a certain % of cases), or a confidence interval. In general, it is best practice to report a confidence interval– this expresses a wider range of uncertainty than a standard error, and it can easily be reported even for effect sizes that can have assymetrical expected error (like Pearson’s r). The use of a 95% confidence interval has become quite typical, but the level of confidence requires thought and judgement.\nIn general, we would report an effect size alongside its confidence interval. So, for example, suppose you measured neurogenesis in adult and stressed mice, finding an average of 3,500 new neurons in control mice and 2,700 new neurons in stressed mice. You would report the mean difference, your key effect size of interest, along with its confidence interval: Mdiff = -800 neurons 95% CI[-1500, -100]. The confidence interval summarizes expected sampling error in your experiment, giving a list of effect sizes that most compatible with your data.\nConfidence intervals help quantify expected sampling variation–the normal differences scientists encounter when drawing random samples from a larger population. Sampling error is typically the least of your problems– you can also miss the truth due to bias and/or measurement error. Thus, you should consider confidence intervals optimistic– they express only one source of uncertainty and do so under an idealized set of conditions that are not typically met in the real world. In fact, in political polling, real error (which can be determined after an election) averages about twice as large as expected sampling error! Conditions within a lab may help limit non-sampling error, so perhaps the polling example is a bit extreme. Still, it is wise to take confidence intervals with a grain of salt.\nAnother thing to remember about confidence intervals is that it is not helpful to make to big of a point about values being outside the confidence interval rather than within. In the example above, we imagined stress effect of neurogenesis: Mdiff = -800 neurons 95% CI[-1500, -100]. Note that confidence interval summarizes a distribution of expected sampling error that stretches out towards infinity in both directions; the 95% CI covers 95% of the area of that distribution. In practice, that means that a reduction of, say -80 neurons is not dramatically less compatible with the data than the upper limit of the confidence interval (-100). So, within reason, consider the boundaries of a confidence interval fairly fuzzy – both because the decline in compatibility is continuous rather than sharp, and because a confidence interval is, as described above, rather optimistic.",
    "crumbs": [
      "Course",
      "Effect Sizes"
    ]
  },
  {
    "objectID": "Effect Sizes.html#more-complex-effect-sizes-more-uncertainty",
    "href": "Effect Sizes.html#more-complex-effect-sizes-more-uncertainty",
    "title": "Effect Sizes",
    "section": "More Complex Effect Sizes, More Uncertainty",
    "text": "More Complex Effect Sizes, More Uncertainty\nResearch programs often grow in complexity over time as we dig into a phenomenon and probe its inner mechanisms. For example, it was once an open question if there is any neurogenesis in adult mammalian brains. At this stage, researchers were interested in simply characterizing neurogenesis: how many (if any) new neurons are produced per day/week/month?\nOnce it was established that there is considerable neurogenesis in the adult brains of lab specieis (perhaps including humans), research questions became more complex as researchers began to look at the factors influencing neurogensis and the mechanisms underlying neurogenesis. For example, researchers began investigating the impact of early-life stress on neurogenesis. This called for examining a difference betwen groups (Mstressed - Mcontrol), a more complex effect size.\nAnd, again, once a factor important to a phenomenon is established, research questions often become even more complex. So, for example, researchers who have found a stress effect on neurogenesis might want to know if this effect is mediated by cortisol. They thus plan a factorial study manipulating both stress and cortisol function. They are now interested in estimating the degree to which the stress effect is reduced when cortisol signaling is eliminated. (See, for example (Mirescu, Peters, and Gould 2004)).\nWhy bring all this up? Because it is important to understand that as effect sizes become more complex, uncertainty increase.\nLet’s examine this for means. For a single mean, the standard error is:\n\nSo let’s say we have been conducting simple studies and find that N = 10 gives us reasonably short confidence intervals. Next, though, we want to take compare groups, and thus estimate the difference between two means. If we keep the sample size the same (N = 10/group), then our standard error will grow (all else being equal):\n\nThat is, assuming everything else stays the same, a difference in means has a standard error 1.4x as big as the standard error for a single group – so we either have to live with that increase in uncertainty or consider a larger sample size.\nAs you might expect, the problem grows and grows as the effect size becomes more complex. If we cant to compare two simple effects (and interaction, or difference on a difference) then we get:\n\nThat is, the expected error for an interaction is 2 times as big as for a single group (assuming sample size and all else stays the same).\nIt gets even worse when we realize that effect sizes often get smaller as questions get more complex. For example, stress may cut neurogenesis by 1/2. At best blocking cortisol might fully restore neurogenesis–but if it is one of many factors, it might only partially restore neurogenesis, say to 3/4 of typical levels. In that case, then, the difference in simple effects (difference in stress effect between normal and cortisol-blocked animals) will be only about 1/4 of typical neurogenesis, a smaller effect that would typically require more samples to study.\nSo - as research questions become more complex, we should expect more uncertainty over smaller effects! That means developing a fruitful line of reserarch is going to be hard. We need either lots of resources, or we need effect sizes so big that even as our questions become more complex we can study them with reasonable sample sizes.\nFrom this discussion there are some important lessons to keep in mind when sample-size planning:\n\nMore complex questions generally require much bigger samples.\nIt is important to make a sample size for your most complex research question. The sample-size needed for a simple difference in means will not be adequate for comparing effects across contexts!\nFruitful science is built on typically built on developing assays with large initial effects (e.g. fear conditioning, LTP) where complex studies can still be achieved with reasonable sample sizes.",
    "crumbs": [
      "Course",
      "Effect Sizes"
    ]
  },
  {
    "objectID": "Effect Sizes.html#two-groups-with-a-quantitative-outcome",
    "href": "Effect Sizes.html#two-groups-with-a-quantitative-outcome",
    "title": "Effect Sizes",
    "section": "Two-Groups With a Quantitative Outcome",
    "text": "Two-Groups With a Quantitative Outcome\nLet’s start with a simple two-group design with a continuous variable. Sticking with my obsession with neurogenesis, let’s imagine you have measured adult neurogenesis in adult mice raised under either stressful or normal conditions. You find a mean of 3,500 neurons in the 6 control animals with a standard deviation of 500. In the 6 animals in the stressed group you find a mean of 2,700 neurons with a standard deviation of 550.\nYou would often reach for a t-test for this analysis. But we can also compute the mean difference between the groups and its confidence interval. Here’s how we would proceed with the statpsych pacakge for R:\n\n\nLoading required package: statpsych\n\n\n                             Estimate       SE         t        df          p\nEqual Variances Assumed:         -800 303.4524 -2.636328 10.000000 0.02489034\nEqual Variances Not Assumed:     -800 303.4524 -2.636328  9.910515 0.02506589\n                                    LL        UL\nEqual Variances Assumed:     -1476.134 -123.8660\nEqual Variances Not Assumed: -1476.962 -123.0376\n\n\nYou can see the effect size (-800) both for when we assume equal variance (as in a standard t-test) and for when we drop that assumption (as is done in a Welch’s t-test). There are good arguments that we should always prefer not to assume equal variance (when variances actually are equal, both approaches yield about the same results, when variances are not equal we really need to analyze it that way… so why not just default to always avoiding the assumption?). So, unless you have specific reasons or pre-registered your analyses, it is a good practice to use the estimate without equal variance assumed.\nFrom this output, we see Mdiff = -800 95% CI[-1476, -123]. Again, we should treat the entire CI (and even values beyond it) as reasonable/compatible with the data (at least in the absence of other evidence).\nNote that statpsych also emits the corresponding t-test, which we could also report. The t-test and the confidence interval have an exact correspondance: p will always be less than .05 when the 95% CI does not include 0 (as is the case here); p will always be &gt;= .05 when the 95% CI does include 0. We could make the same statements about the p boundary of .01 and a 99% CI, and .1 with a 90% CI, etc. What this means, overall, is that the test and the estimate are from the same model, same data, and will always have corresponding results: from the confidence interval you know if the finding is statistically significant; from the p value you know if 0 is in the confidence interval. Because the confidence interval tells you about significance and more there are many who believe we could get by with only reporting the effect size and confidence interval.",
    "crumbs": [
      "Course",
      "Effect Sizes"
    ]
  },
  {
    "objectID": "Effect Sizes.html#two-groups-with-a-quantitative-outcome-compare-medians-rather-than-means",
    "href": "Effect Sizes.html#two-groups-with-a-quantitative-outcome-compare-medians-rather-than-means",
    "title": "Effect Sizes",
    "section": "Two Groups with a Quantitative Outcome: Compare Medians Rather than Means",
    "text": "Two Groups with a Quantitative Outcome: Compare Medians Rather than Means\nNeurogenesis experiments yield neuron counts, and counts are typically skewed, violating the assumptions for a t-test. In these cases, researchers often reach for a non-parametric test. We can do that, too, comparing the medians between both groups (there’s even a good argument that we should almost always do this… but set that aside for now).\nTo compare medians with statpsych we need the raw data. In the example below I’ve generated fake raw data for a typical neurogenesis experiment and then used statpsych to estimate the median difference betwen groups:\n\n\n Median1 Median2 Median1-Median2      SE        LL       UL\n    2756  3116.5          -360.5 448.298 -1239.148 518.1479\n\n\nHere we see obtain a median difference: -361 neurson 95% CI[-1239, 518]. In this fake data, the effect of stress is more ambiguous – the CI is compatible with large decreases in neurogenesis, but also with small declines, no change at all, and even a modest increase. It would take more data to more clearly determine the typical difference. Fortunately, effect sizes can be synthesized through meta-analysis, lettings us use multiple small studies to narrow in on the truth (as long as we can get an unbiased collection of studies to synthesize).\nComparison of median differences roughly corresponds to a Mann-Whitney U, but not perfectly, as the mean difference does to the t-test.",
    "crumbs": [
      "Course",
      "Effect Sizes"
    ]
  },
  {
    "objectID": "Effect Sizes.html#two-groups-with-a-quantitative-outcome-express-effect-sizes-in-different-ways",
    "href": "Effect Sizes.html#two-groups-with-a-quantitative-outcome-express-effect-sizes-in-different-ways",
    "title": "Effect Sizes",
    "section": "Two Groups with a Quantitative Outcome: Express Effect Sizes in Different Ways",
    "text": "Two Groups with a Quantitative Outcome: Express Effect Sizes in Different Ways\nEffect sizes are quantitative. It is not surprising that quantities can be re-expressed in different units (e.g. length can be expressed in feet, inches, meters, etc.). The same is true for effect sizes: we normally express them in the same units as measured (number of neurons, in our example), but can re-express them in different units.\nFor example, we can express a mean difference as a mean ratio– quantifying the ratio between the control and treated groups. Ditto for median differences. Here’s the code for median differences in statpsych (using the same fake data as in the difference of medians example above):\n\n\n Median1 Median2 Median1/Median2        LL       UL\n    2756  3116.5       0.8843254 0.6524854 1.198542\n\n\nWe see that stressed animals had 88% of the neurogenesis of controls, 95% CI[65%, 120%].\nNote that even if we change the units, our interpration shouldn’t change (someone who is unusually tall in meters should still be considered unusually tall measured in feet). So, just as we obseved ambiguous results when thinking about the difference in medians, we have the same reaction to the ratio: this data is compatible with large reductions in neurogenesis, small reductions, no change, and even a modest increase.\nOne of the most popular transformations is to express mean differences as standardized mean differences, dividing them by the standard deviation to put them in standard deviation units (Cohen’s d). Here’s the code in statpsych for the mean difference example above:\n\n\n                          Estimate adj Estimate        SE        LL          UL\nUnweighted standardizer: -1.522085    -1.405001 0.7189458 -2.931193 -0.11297695\nWeighted standardizer:   -1.522085    -1.405001 0.6702018 -2.835656 -0.20851341\nGroup 1 standardizer:    -1.454545    -1.224880 0.7595127 -2.943163  0.03407204\nGroup 2 standardizer:    -1.600000    -1.347368 0.8354639 -3.237479  0.03747924\n\n\nNote the output is a bit more complex– that’s because what seems like the simple instruction to “divide by the standard deviation” is a bit complex. Should we average just average the two groups standard deviations (top line)? Or should we pool the standard deviations based on sample size (second line)? Or should we use one of the groups as our standardized (lines 3 and 4)? Given this complexity (and others), it is usually best to stick with raw-score effect sizes. When you do want to use/interpret Cohen’s d, the average sd (top line) corresponds to the assumptions in a Welch’s t-test, and so is probably the best default option.\nIn this example, we see that stress reduced neurogenesis: davg = -1.5 95% CI[-2.9, -0.11]. Just as with the raw-score, we see this is a very large effect, but with considerable uncertainty–the daa are compatible with a huge decline (3 standard deviations, almost no overlap in distributions between the groups), modest declines, and even fairly subtle declines (just over a tenth of a standard deviation). Note the 95% CI does exclude 0, so we know this we could reject the null of exactly 0 at a stringency of .05 – but that doesn’t mean that we’ve demonstrated an effect that would be easy to study further!",
    "crumbs": [
      "Course",
      "Effect Sizes"
    ]
  },
  {
    "objectID": "Effect Sizes.html#complex-designs-effect-sizes-and-confidence-intervals-for-an-interaction",
    "href": "Effect Sizes.html#complex-designs-effect-sizes-and-confidence-intervals-for-an-interaction",
    "title": "Effect Sizes",
    "section": "Complex Designs; Effect Sizes and Confidence Intervals for An Interaction",
    "text": "Complex Designs; Effect Sizes and Confidence Intervals for An Interaction\nAs research questions become more complex, we turn to factorial designs, in which more than one independent variable is manipulated. Current norms are to analyze factorial designs with an ANOVA, generating F tests for the main effects and interactions. Each F test has an effect size associated with it known as eta-squared, the percentage of variance accounted for. More often than not, however, the researcher’s main research questions are about magnitude of mean or median differences across cells. Specifically, complex designs are usually with the goal of knowing how much one independent variable influences the effect of the other. We answer this question with the difference on the difference, the difference between simple effects. It sounds a bit complicated, but it is simple in practice.\nContinuing with our neurogenesis example, imagine you completed a factorial study to determine the role of cortisol in mediating the effects of stress on neurogenesis. You will raise mice under stressful conditions or normal conditions. In addition, 1/2 the mice in each rearing condition will have their adrenal glands removed at a young age; the other 1/2 will undergo a sham surgery. You’ll run 6 animals per cell in this 2x2 factorial design.\nBelow is some fake data and how we can estimate effect sizes for mean differences with statpsych:\n\n\n[1] 3153.667\n\n\n[1] 2717.167\n\n\n[1] 2856.833\n\n\n[1] 3316.833\n\n\n          Estimate       SE           t        df          p         LL\nAB:      -896.5000 419.1506 -2.13884916 17.392502 0.04691144 -1779.3127\nA:         11.7500 209.5753  0.05606576 17.392502 0.95592847  -429.6564\nB:       -151.4167 209.5753 -0.72249283 17.392502 0.47959455  -592.8230\nA at b1: -436.5000 289.1121 -1.50979500  7.543165 0.17179077 -1110.2877\nA at b2:  460.0000 303.4822  1.51573956  9.997743 0.16054463  -216.2212\nB at a1: -599.6667 335.2416 -1.78875977  9.724923 0.10478869 -1349.5053\nB at a2:  296.8333 251.5956  1.17980342  8.420328 0.27033500  -278.3444\n                 UL\nAB:       -13.68725\nA:        453.15637\nB:        289.98971\nA at b1:  237.28765\nA at b2: 1136.22121\nB at a1:  150.17201\nB at a2:  872.01109\n\n\nThis is a lot of output to digest. We’ve defined the manipulation of cortisol as variable B and the manipulation of stress as Variable C (see documentaiton from statpsych). Let’s focus on the simple effects of stress (variable A):\n\nFor sham operated animals (b1), there is a large but uncertain effect of stress: Mstress_with_cortisoal = -437 95% CI [-1110, 237]\nFor adrenelectomized animals (b2), animals in the stressed condition had higher levels of neurogenesis than controls: Mstress_effect_without_cortisol = 460 95% CI[-216, 1136]\nThus, adrenelectomy transformed the effect of stress, turning it from a -437 neuron decrement to a 460 increase, a large though somewhat uncertain interaction: M⍙⍙ = -897 95% CI [-1779, -14]. Note that this interaction effect size has a p value – this will correspond to the p value for the test of this 2x2 interaction in an ANOVA.\n\nWe’ve focused here on comparing means – but we could also do the same analysis of a complex design while comparing medians (see statpsych::ci.2x2.median.bs). And, for categorical outcomes we can compare proportions (see statpsych::ci.2x2.prop.bs). And while this example uses a fully-between subjects design, we can obtain the same effect sizes estimates for within-subjects and mixed designs (e.g. see statpsych::ci.2x2.mean.ws and statpsych::ci.2x2.mean.mixed).",
    "crumbs": [
      "Course",
      "Effect Sizes"
    ]
  },
  {
    "objectID": "Effect Sizes.html#lots-of-other-effect-sizes",
    "href": "Effect Sizes.html#lots-of-other-effect-sizes",
    "title": "Effect Sizes",
    "section": "Lots of Other Effect Sizes",
    "text": "Lots of Other Effect Sizes\nThe statpsych package can help you obtain effect sizes for lots of additional research designs. Here’s a sampler:\n\n\n\nci.2x2.mean.bs()\nComputes tests and confidence intervals of effects in a 2x2 between-subjects design for means\n\n\nci.2x2.mean.mixed()\nComputes tests and confidence intervals of effects in a 2x2 mixed design for means\n\n\nci.2x2.mean.ws()\nComputes tests and confidence intervals of effects in a 2x2 within-subjects design for means\n\n\nci.2x2.median.bs()\nComputes tests and confidence intervals of effects in a 2x2 between-subjects design for medians\n\n\nci.2x2.median.mixed()\nComputes confidence intervals of effects in a 2x2 mixed design for medians\n\n\nci.2x2.median.ws()\nComputes confidence intervals of effects in a 2x2 within-subjects design for medians\n\n\nci.2x2.prop.bs()\nComputes tests and confidence intervals of effects in a 2x2 between- subjects design for proportions\n\n\nci.2x2.prop.mixed()\nComputes tests and confidence intervals of effects in a 2x2 mixed factorial design for proportions\n\n\nci.2x2.stdmean.bs()\nComputes confidence intervals of standardized effects in a 2x2 between-subjects design\n\n\nci.2x2.stdmean.mixed()\nComputes confidence intervals of standardized effects in a 2x2 mixed design\n\n\nci.2x2.stdmean.ws()\nComputes confidence intervals of standardized effects in a 2x2 within-subjects design\n\n\nci.agree.3rater()\nComputes confidence intervals for a 3-rater design with dichotomous ratings\n\n\nci.agree()\nConfidence interval for a G-index of agreement\n\n\nci.agree2()\nConfidence interval for G-index difference in a 2-group design\n\n\nci.bayes.normal()\nBayesian credible interval for a normal prior distribution\n\n\nci.bayes.prop()\nBayesian credible interval for a proportion\n\n\nci.biphi()\nConfidence interval for a biserial-phi correlation\n\n\nci.bscor()\nConfidence interval for a biserial correlation\n\n\nci.cod()\nConfidence interval for a coefficient of dispersion\n\n\nci.condslope.log()\nConfidence intervals for conditional (simple) slopes in a logistic model\n\n\nci.condslope()\nConfidence intervals for conditional (simple) slopes in a linear model\n\n\nci.cor.dep()\nConfidence interval for a difference in dependent Pearson correlations\n\n\nci.cor()\nConfidence interval for a Pearson or partial correlation\n\n\nci.cor2.gen()\nConfidence interval for a 2-group correlation difference\n\n\nci.cor2()\nConfidence interval for a 2-group Pearson correlation difference\n\n\nci.cqv()\nConfidence interval for a coefficient of quartile variation\n\n\nci.cramer()\nConfidence interval for Cramer’s V\n\n\nci.cronbach()\nConfidence interval for a Cronbach reliability\n\n\nci.cronbach2()\nConfidence interval for a difference in Cronbach reliabilities in a 2-group design\n\n\nci.cv()\nConfidence interval for a coefficient of variation\n\n\nci.etasqr()\nConfidence interval for eta-squared\n\n\nci.fisher()\nFisher confidence interval\n\n\nci.indirect()\nConfidence interval for an indirect effect\n\n\nci.kappa()\nConfidence interval for two kappa reliability coefficients\n\n\nci.lc.gen.bs()\nConfidence interval for a linear contrast of parameters in a between-subjects design\n\n\nci.lc.glm()\nConfidence interval for a linear contrast of general linear model parameters\n\n\nci.lc.mean.bs()\nConfidence interval for a linear contrast of means in a between-subjects design\n\n\nci.lc.median.bs()\nConfidence interval for a linear contrast of medians in a between-subjects design\n\n\nci.lc.prop.bs()\nConfidence interval for a linear contrast of proportions in a between- subjects design\n\n\nci.lc.reg()\nConfidence interval for a linear contrast of regression coefficients in multiple group regression model\n\n\nci.lc.stdmean.bs()\nConfidence interval for a standardized linear contrast of means in a between-subjects design\n\n\nci.lc.stdmean.ws()\nConfidence interval for a standardized linear contrast of means in a within-subjects design\n\n\nci.mad()\nConfidence interval for a mean absolute deviation\n\n\nci.mann()\nConfidence interval for a Mann-Whitney parameter\n\n\nci.mape()\nConfidence interval for a mean absolute prediction error\n\n\nci.mean.fpc()\nConfidence interval for a mean with a finite population correction\n\n\nci.mean.ps()\nConfidence interval for a paired-samples mean difference\n\n\nci.mean() ci.mean1()\nConfidence interval for a mean\n\n\nci.mean2()\nConfidence interval for a 2-group mean difference\n\n\nci.median.ps()\nConfidence interval for a paired-samples median difference\n\n\nci.median() ci.median1()\nConfidence interval for a median\n\n\nci.median2()\nConfidence interval for a 2-group median difference\n\n\nci.oddsratio()\nConfidence interval for an odds ratio\n\n\nci.pairs.mult()\nConfidence intervals for pairwise proportion differences of a multinomial variable\n\n\nci.pairs.prop.bs()\nBonferroni confidence intervals for all pairwise proportion differences in a between-subjects design\n\n\nci.pbcor()\nConfidence intervals for point-biserial correlations\n\n\nci.phi()\nConfidence interval for a phi correlation\n\n\nci.poisson()\nConfidence interval for a Poisson rate\n\n\nci.popsize()\nConfidence interval for an unknown population size\n\n\nci.prop.fpc()\nConfidence interval for a proportion with a finite population correction\n\n\nci.prop.inv()\nConfidence interval for a proportion using inverse sampling\n\n\nci.prop.ps()\nConfidence interval for a paired-samples proportion difference\n\n\nci.prop() ci.prop1()\nConfidence intervals for a proportion\n\n\nci.prop2.inv()\nConfidence interval for a 2-group proportion difference using inverse sampling\n\n\nci.prop2()\nConfidence interval for a 2-group proportion difference\n\n\nci.pv()\nConfidence intervals for positive and negative predictive values with retrospective sampling\n\n\nci.random.anova()\nConfidence intervals for parameters of one-way random effects ANOVA\n\n\nci.ratio.cod2()\nConfidence interval for a ratio of dispersion coefficients in a 2-group design\n\n\nci.ratio.cv2()\nConfidence interval for a ratio of coefficients of variation in a 2-group design\n\n\nci.ratio.mad.ps()\nConfidence interval for a paired-sample MAD ratio\n\n\nci.ratio.mad2()\nConfidence interval for a 2-group ratio of mean absolute deviations\n\n\nci.ratio.mape2()\nConfidence interval for a ratio of mean absolute prediction errors in a 2-group design\n\n\nci.ratio.mean.ps()\nConfidence interval for a paired-samples mean ratio\n\n\nci.ratio.mean2()\nConfidence interval for a 2-group mean ratio\n\n\nci.ratio.median.ps()\nConfidence interval for a paired-samples median ratio\n\n\nci.ratio.median2()\nConfidence interval for a 2-group median ratio\n\n\nci.ratio.poisson2()\nConfidence interval for a ratio of Poisson rates in a 2-group design\n\n\nci.ratio.prop.ps()\nConfidence interval for a paired-samples proportion ratio\n\n\nci.ratio.prop2()\nConfidence interval for a 2-group proportion ratio\n\n\nci.ratio.sd2()\nConfidence interval for a 2-group ratio of standard deviations\n\n\nci.rel2()\nConfidence interval for a 2-group reliability difference\n\n\nci.reliability()\nConfidence interval for a reliability coefficient\n\n\nci.rsqr()\nConfidence interval for squared multiple correlation\n\n\nci.sign()\nConfidence interval for the parameter of the one-sample sign test\n\n\nci.slope.mean.bs()\nConfidence interval for the slope of means in a one-factor experimental design with a quantitative between-subjects factor\n\n\nci.slope.prop.bs()\nConfidence interval for a slope of a proportion in a single-factor experimental design with a quantitative between-subjects factor\n\n\nci.spcor()\nConfidence interval for a semipartial correlation\n\n\nci.spear()\nConfidence interval for a Spearman correlation\n\n\nci.spear2()\nConfidence interval for a 2-group Spearman correlation difference\n\n\nci.stdmean.ps()\nConfidence intervals for a paired-samples standardized mean difference\n\n\nci.stdmean() ci.stdmean1()\nConfidence interval for a standardized mean\n\n\nci.stdmean.strat()\nConfidence intervals for a 2-group standardized mean difference with stratified sampling\n\n\nci.stdmean2()\nConfidence intervals for a 2-group standardized mean difference\n\n\nci.tetra()\nConfidence interval for a tetrachoric correlation\n\n\nci.theil()\nTheil-Sen estimate and confidence interval for slope\n\n\nci.tukey()\nTukey-Kramer confidence intervals for all pairwise mean differences in a between-subjects design\n\n\nci.var.upper()\nUpper confidence limit of a variance\n\n\nci.yule()\nConfidence intervals for generalized Yule coefficients",
    "crumbs": [
      "Course",
      "Effect Sizes"
    ]
  },
  {
    "objectID": "Effect Sizes.html#exercises",
    "href": "Effect Sizes.html#exercises",
    "title": "Effect Sizes",
    "section": "Exercises",
    "text": "Exercises\n\nAnush will compare expression of egr1 between control animals and those with a conditonal knockout of creb1. He will conduct a simple two-group experiment and his primary outcome (gene expression) is quantitative. &lt;div class=‘webex-radiogroup’ id=‘radio_RTSDXCHXFG’&gt;&lt;label&gt;&lt;input type=“radio” autocomplete=“off” name=“radio_RTSDXCHXFG” value=““&gt;&lt;/input&gt; &lt;span&gt;This is a biased approach that capitilizes on chance&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=”radio” autocomplete=“off” name=“radio_RTSDXCHXFG” value=“answer”&gt;&lt;/input&gt; &lt;span&gt;Smart work, Joe, no wonder you always confirm your hypotheses!&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\nAnisha wants to run a single-cell transcriptomics experiment. She finds a paper that runs 3 biological replicates. She decides to use the same sample size. What should we make of this approach? &lt;div class=‘webex-radiogroup’ id=‘radio_WZMSQSHDJO’&gt;&lt;label&gt;&lt;input type=“radio” autocomplete=“off” name=“radio_WZMSQSHDJO” value=““&gt;&lt;/input&gt; &lt;span&gt;Sadly, the published literature is not always a good guide -- many fields regularly use inadquate sample sizes; Anisha needs more information to know if this is reasonable&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=”radio” autocomplete=“off” name=“radio_WZMSQSHDJO” value=“answer”&gt;&lt;/input&gt; &lt;span&gt;Sweet; as long as you have a citation to back you up, you're fine!&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;",
    "crumbs": [
      "Course",
      "Effect Sizes"
    ]
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "An Introduction to How Much Data",
    "section": "",
    "text": "The topic of this course is sample-size planning: How do you know how much data to collect for a research project? \nStrangely, this is not often a point of emphasis in our scientific training. It is common for even advanced graduate courses in statistics to omit this topic or touch on it rarely. Even worse, the informal training you might receive in a lab is often wrong.\nThe lack of strong training on sample-size determination in science is worrisome. You wouldn’t get into a plane with a pilot who didn’t have a flight plan. So it seems a bit strange that often we charge off into experimentation with no real sense of where we are going and when it will end. That’s a missed opportunity, because with just a bit of initial planning we can do research that is more fruitful and reproducible. \nThe goal of this workshop is to dymystify sample-size planning and to provide actionable, practical advice about how to develop solid sample-size plans for your research. This course was developed not by a professional statistician but by a neuroscientist who accidentally fell down the rabbit hole of caring deeply about statistical inference. Thus, the course hopes to present material clearly, in language fellow bench-scientists can understand without having to delve too deeply into statistical arcana.   \nHere’s the outline of the course:\n\nFirst, we’ll discuss why sample-size planning is so vital to good science and therefore why it is worth investing your time and energy into completing this workshop. You may have been driven to this class because of some external mandate imposed by a funder or training program. The goal for this section is to help develop your own, intrinsic motivation for learning how to plan sample sizes.\nNext, we need to clear some very common but very bad practices: we’ll discuss the perils of “run-and-check” and of just copying forward sample-sizes from previous studies. It turns out these informal approaches to sample-size determination undermine sound science.\nThe unit on effect sizes provides some of the foundational knowledge needed to understand sample-size planning. Specifically, we define effect sizes, discuss how they can be expressed in different units, and provide some tips on how to build “effect size zoos” and start thinking in effect sizes.\nFinally, we get to the actual nitty-gritty of sample-size planning, covering three major approaches:\n\nPlanning for Power\nPlanning for Precision\nPlanning for Evidence (still under development)\n\nWith an understanding of some of the different approaches to sample-size planning, we’re ready for a check-list for a quality sample-size plan and to review some model plans.\nLast but not least, this workshop describes some important strategies for dealing with sample-size sticker shock. There are lots of ways to optimize your experiments to help reduce your sample-size needs and get more information out of your experiments.\n\nHope you enjoy this workshop. Please submit questions or bug reports to the GitHub page for this course: https://github.com/rcalinjageman/How-Much-Data/discussions.\nThe development of this course was supported by the NIGMS division of the National Institute of Health through grant 5R25GM132784-02. You can find other training modules focused on Rigor and Reproducibility here: https://www.nigms.nih.gov/training/pages/clearinghouse-for-training-modules-to-enhance-data-reproducibility.aspx.",
    "crumbs": [
      "Course",
      "Introduction"
    ]
  },
  {
    "objectID": "Model Sample-Size Plans.html",
    "href": "Model Sample-Size Plans.html",
    "title": "Model Sample-Size Plans",
    "section": "",
    "text": "The video above walks through model sample size plans when planning for power and for precision.\nIf this workshop helps inform your own sample-size plan, please share it with our community! Submit your sample size plans to the GitHub discussion page for this course, here: https://github.com/rcalinjageman/How-Much-Data/discussions. When you do, please indicated if you are willing for your sample-size plan to be linked/featured/discussed from this page as well.\nWhile working on your sample-size plan,it can also be helpful to reflect on what not to do. You may find this commentary on sample-size reporting in Nature Neuroscience useful (Goodhill 2017). Sadly, Nature Neuroscience did not seem to feel it was worth alerting its readers to these issues.\n\n\n\n\nReferences\n\nGoodhill, Geoffrey J. 2017. “Is Neuroscience Facing up to Statistical Power?” arXiv Preprint, January, 1–5. http://arxiv.org/abs/1701.01219.",
    "crumbs": [
      "Course",
      "Model Sample-Size Plans"
    ]
  },
  {
    "objectID": "Planning for Evidence.html",
    "href": "Planning for Evidence.html",
    "title": "Planning for Evidence",
    "section": "",
    "text": "Bayesian statistics offers a very different path for sample-size determination: Planning for evidence.\nOne especially compelling version of this approach is called open-ended Sequential Bayes Factor (SBF) design, in which the researcher collects data until compelling evidence emerges for their hypothesis or the null.\nDespite the best-laid plans, a module on planning for evidence is not ready for release yet. Please stay tuned. In the meantime, the most authoritative and accessible resources for this approach is (Schönbrodt and Wagenmakers 2017).\n\n\n\n\nReferences\n\nSchönbrodt, Felix D., and Eric-Jan Wagenmakers. 2017. “Bayes Factor Design Analysis: Planning for Compelling Evidence.” Psychonomic Bulletin & Review, 1–16. https://doi.org/10.3758/s13423-017-1230-y.",
    "crumbs": [
      "Course",
      "Three Approaches",
      "Planning for Evidence"
    ]
  },
  {
    "objectID": "Planning for Precision.html",
    "href": "Planning for Precision.html",
    "title": "Planning for Precision",
    "section": "",
    "text": "The video above gives an overview of planning for precision. Now let’s dig in with an example.\nIn the power for planning section, we imagined conducting a study of the effect of stress on neurogenesis in the hippocampus. The plan is to raise mice under stressful or standard conditions and then, as adults, characterize neurogenesis by injecting BRDU and counting labelled cells 2 hours after injection (This example is inspired by [@mirescu2004]).\nTo plan for power, we need a lot of inputs: we’d need to know the type of test we’ll use, its stringency, the power level desired, the typical level of neurogenesis in the control group, a quantitative prediction about the degree to which stress will change neurogenesis, and the expected standard deviation within each group. This long list of inputs can feel frustrating – that’s because planning for power is for testing a hypothesis, and until you have a well-developed hypothesis it is simply not possible to make a good sample plan.\nPlanning for precision can help – the goal of planning for precision is to conduct a decriptive study, one where we simply want to provide a quality measurement. That is, we’re not trying to confirm a specific theory about neurogenesis (stress halts neurogenesis); we’re trying to accurately measure the impact of stress on neurogenesis. Planning for precision lets us weigh our expected sampling error against resources: how close to the truth can you afford to get?\nPlanning for precision requires fewer inputs than planning for power. You need:\nSetting the confidence level is pretty easy. Knowing the variety of scores to expect within a group can present some challenges, but is not too hard to develop some good guesses. Critically, standard deviations in the published literature are likely not very biased because they are not a major point of selection/filtering. So, perhaps you find previous BRDU experiments in the same strain of mice and find a typical standard deviation is 500 neurons.\nThe last input can is the most difficult: how much uncertainty can you tolerate? We need a number: are you ok with an answer within 100 neurons of the truth? 500 neurons of the truth? 1,000 neurons of the truth? The answer will be informed by what you can afford and also by your expectations of the magnitude of the effect. In political polling, pollsters want to get small margins of error in races expected to be close; they can tolerate large margins of error in non-competitive races. Similarly, if you have reason to believe that stress will make an enormous impact on neurogenesis, then you might be comfortable with a fairly long confidence interval. If you believe the effect will be subtle, you’d rather have a short confidence interval. Remember, the goal is to get a reasonable reading on the effect; to describe it well to inform your theories and hypotheses – you don’t yet need to be so precise as to test a hypothesis, but you still want to generate useful knowledge.\nIf you have expectations about the control group, you might be able to think in terms of percentages. For example, if you know that typical neurogenesis is 3,500 new neurons, you could think about getting a confidence interval that spans 10% (350 neurons), 20% (750 neurons), etc.\nMost likely you will do some exploration and iteration – you might initially want a very short confidence interval, but then scale back to find a reasonable compromise between precision and cost.\nFor this example, let’s check out confidence interval lengths of 250 neurons (1/2 a standard deviation long), 500 (1 standard deviation long), and 1,000 (2 standard deviations long) and we can then think about the costs and pick a precision level.\nTo get our sample-size needs we will again turn to statpsych in R. For this two-group design we will use the size.ci.mean2. This function along with all other functions in statpsych are documented here: https://dgbonett.github.io/statpsych/reference/index.html. The parameters for this function are:\nHere’s our code, starting with a desired CI width of 250 neurons (entire CI length is just half the standard deviation of 500 neurons).\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\nLoading required package: statpsych\n\nstatpsych::size.ci.mean2(\n  alpha = 1 - 0.95,    # alpha of .05 for 95% CI\n  var = 500^2,         # sd is 500, so we enter 500^2 for variance\n  w = 250,             # we'll start with a CI length of 250\n  R = 1                # R = 1 indicates equal sample sizes\n)\n\n  n1  n2\n 124 124\nWhoa! Getting that much prevision is expensive. Let’s consider something a bit less ambitious:\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\nstatpsych::size.ci.mean2(\n  alpha = 1 - 0.95,    \n  var = 500^2,         \n  w = 500,             # update CI length to 500\n  R = 1                \n)\n\n n1 n2\n 32 32\nMuch better! That’s a lot less animals, though we will also be a lot less certain about the effects of stress. Let’s see what happens if we’re willing to be even more uncertain:\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\nstatpsych::size.ci.mean2(\n  alpha = 1 - 0.95,    \n  var = 500^2,         \n  w = 1000,             # update CI length to 500\n  R = 1                \n)\n\n n1 n2\n  9  9\nThat’s a somewhat large, but not atypical sample size for these types of studies. But it is also not especially precise: we should expect our typical confidence interval to be 1,000 neurons wide. Given that within-group variability is 500, that’s a pretty long interval, something only suitable if we believe stress will have a pretty enormous impact on neurogenesis.\nWhile planning for precision takes fewer inputs, it still often reveals that more resources are needed than have been typical in a field. Part of that is just the sad truth: many assays are regularly conducted with sample-sizes that are indefensibly small, and we need to change that. But there are other ways to deal with this sticker-shock; see the section on Decreasing Sample-Size Needs.",
    "crumbs": [
      "Course",
      "Three Approaches",
      "Planning for Precision"
    ]
  },
  {
    "objectID": "Planning for Precision.html#important-considerations-when-planning-for-precision",
    "href": "Planning for Precision.html#important-considerations-when-planning-for-precision",
    "title": "Planning for Precision",
    "section": "Important Considerations When Planning for Precision",
    "text": "Important Considerations When Planning for Precision\n\nWhat if your inputs are off? That, is what if variation is 20% higher than you expected? Or 50%? The best practice is to explore a variety of inputs and judiciously choose a sample size.\nDoes your sample-size plan matches your research question? It sounds obvious, but you need to make sure you are planning for power for the analysis that answers your research quesiton. For example, suppose you describe the effects of both stress (high/low) and sex (male/female) on neurogensis. You will probably want to describe how much stress affects neurogenesis in male mice. And you will probably want to describe how much stress affects neurogenesis in female mice. But most importantly, you will probably want to describe the difference between these simple effects, the extent to which stress effects differ between the sexes. In that case, you need to make sure your sample-size plan is for the confidence interval for the difference between the simple effects (the interaction), and effect size that has much higher sampling error than either simple effect.\nWill you estimate multiple parameters? When you want to estimate multiple parameters you increase the overall risk of providing a confidence interval that doesn’t capture the true parameter for the population. You thus need to make a sample size plan that includes the increased needs that occur when you make multiple estimates. This can become complicated, and sometimes can only be solved through simulation.\nDo you have non-independent/hierarchical data? Hierarchical data (e.g. multiple cells recorded from the each animal in each condition) violates the non-independence assumed in most statistical tests. You will need to make sure you have a porper analysis strategy (e.g. sufficient summary statistics approach) and a power plan that matches.",
    "crumbs": [
      "Course",
      "Three Approaches",
      "Planning for Precision"
    ]
  },
  {
    "objectID": "Planning for Precision.html#planning-for-power-for-other-designs",
    "href": "Planning for Precision.html#planning-for-power-for-other-designs",
    "title": "Planning for Precision",
    "section": "Planning for Power for Other Designs",
    "text": "Planning for Power for Other Designs\nstatpsych has a wide range of functions for planning for precision for simple designs (https://dgbonett.github.io/statpsych/reference/index.html). These include:\n\n\n\nsize.ci.condmean()\nSample size for a conditional mean confidence interval\n\n\nsize.ci.cor2()\nSample size for a 2-group Pearson correlation difference confidence interval\n\n\nsize.ci.cronbach2()\nSample size for a 2-group Cronbach reliability difference confidence interval\n\n\nsize.ci.etasqr()\nSample size for an eta-squared confidence interval\n\n\nsize.ci.indirect()\nSample size for an indirect effect confidence interval\n\n\nsize.ci.lc.ancova()\nSample size for a linear contrast confidence interval in an ANCOVA\n\n\nsize.ci.lc.mean.bs()\nSample size for a between-subjects mean linear contrast confidence interval\n\n\nsize.ci.lc.mean.ws()\nSample size for a within-subjects mean linear contrast confidence interval\n\n\nsize.ci.lc.prop.bs()\nSample size for a between-subjects proportion linear contrast confidence interval\n\n\nsize.ci.lc.stdmean.bs()\nSample size for a between-subjects standardized linear contrast of means confidence interval\n\n\nsize.ci.lc.stdmean.ws()\nSample size for a within-subjects standardized linear contrast of means confidence interval\n\n\nsize.ci.mape()\nSample size for a mean absolute prediction error confidence interval\n\n\nsize.ci.mean.prior()\nSample size for a mean confidence interval using a planning value from a prior study\n\n\nsize.ci.mean.ps()\nSample size for a paired-samples mean difference confidence interval\n\n\nsize.ci.mean()\nSample size for a mean confidence interval\n\n\nsize.ci.mean2()\nSample size for a 2-group mean difference confidence interval\n\n\nsize.ci.pbcor()\nSample size for a point-biserial correlation confidence interval\n\n\nsize.ci.prop.prior()\nSample size for a proportion confidence interval using a planning value from a prior study\n\n\nsize.ci.prop.ps()\nSample size for a paired-sample proportion difference confidence interval\n\n\nsize.ci.prop()\nSample size for a proportion confidence interval\n\n\nsize.ci.prop2()\nSample size for a 2-group proportion difference confidence interval\n\n\nsize.ci.ratio.mean.ps()\nSample size for a paired-samples mean ratio confidence interval\n\n\nsize.ci.ratio.mean2()\nSample size for a 2-group mean ratio confidence interval\n\n\nsize.ci.ratio.prop.ps()\nSample size for a paired-samples proportion ratio confidence interval\n\n\nsize.ci.ratio.prop2()\nSample size for a 2-group proportion ratio confidence interval\n\n\nsize.ci.rsqr()\nSample size for a squared multiple correlation confidence interval\n\n\nsize.ci.slope()\nSample size for a slope confidence interval\n\n\nsize.ci.spear()\nSample size for a Spearman correlation confidence interval\n\n\nsize.ci.spear2()\nSample size for a 2-group Spearman correlation difference confidence interval\n\n\nsize.ci.stdmean.ps()\nSample size for a paired-samples standardized mean difference confidence interval\n\n\nsize.ci.stdmean2()\nSample size for a 2-group standardized mean difference confidence interval\n\n\n\nOne thing you’ll notice that’s missing is interactions for complex designs – these are currently not available in statpsych. Stay tuned!",
    "crumbs": [
      "Course",
      "Three Approaches",
      "Planning for Precision"
    ]
  },
  {
    "objectID": "posts/statpsych-for-sample-size-planning/index.html",
    "href": "posts/statpsych-for-sample-size-planning/index.html",
    "title": "statpsych for Sample-Size Planning",
    "section": "",
    "text": "One great tool for sample-size planning is the statpsych package by Doug Bonett.\nThere are loads and loads of goodies in statpsych, and the documentation is excellent and authoritative.\nHere, for example, is how you can find the sample-size needed to obtain a desired level of power for a predicted effect in a 2-group design\n\n# If you don't have statpsych installed, install it!\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\n\nLoading required package: statpsych\n\n# size.test.mean2 gives power for the mean difference in a 2-group design\n# parameters are alpha level, desired power, average within-group variance, effect size, and ratio of group1 to group2 sample sizes\n# Full documentation is at: https://dgbonett.github.io/statpsych/reference/size.test.mean2.html\n# In this example, we have \n#  * an alpha of .05, \n#  * desired power of .95, \n#  * expected average variance of 100 (sd_avg = 10)\n#  * an predicted effect size of 10\n#  * and equal sample-sizes in both groups (ratio of 1 between n1 and n2)\n# The output shows that we need n = 27 per group.\nstatpsych::size.test.mean2(.05, .95, 100, 10, 1) \n\n n1 n2\n 27 27\n\n# We can easiy investigate other effect sizes, power levels, etc.  For example:\n#  Here we explore effect sizes of 5, 10, and 20 while keeping\n#  all other parameters constant\nfor(e_size in c(5, 10, 20)){\n  print(\n    paste(\n      \"For effect size of\", e_size,\n      \"need sample size of\", \n      paste(statpsych::size.test.mean2(.05, .95, 100, e_size, 1), collapse = \", \")\n    )\n  )\n  \n}\n\n[1] \"For effect size of 5 need sample size of 105, 105\"\n[1] \"For effect size of 10 need sample size of 27, 27\"\n[1] \"For effect size of 20 need sample size of 8, 8\""
  },
  {
    "objectID": "Resources.html",
    "href": "Resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here are some resources I found useful for developing this workshop. I’ll do my best to keep it updated. If you come across a resource you feel is helpful, submit it on the GitHub discussion board for this workshop (https://github.com/rcalinjageman/esci/discussions) and I’ll add it to this list and feature it in a blog post.\n\nGuidelines and Regulations Related to Sample-Size Planning\nOn important reason to plan your sample-size in advance is stakeholders in the life sciences are increasingly requiring evidence that sample-sizes are adequate\n\nNIH\n\nAs of 2016, NIH has adopted a new initiative on Rigor and Reproducibility that stress evaluation of project proposals for their ability to produce robust and unbiased results.\nIn explaining this new policy, sample-size planning was listed as a way to help meet this new evaluation criterion. See the NIH blog post here:\n\nReporting Guidelines - NIH also helped organize a set of principles for the reporting of pre-clinical research; these guidelines were endorsed by a wide variety of journals and professional societies.\n\nHere are the NIH guidelines. The guidelines related to transparency stipulates that authors should explain their sample-size determinations.\nMany journals either already enforced these guidelines are have updated their author instructions to do so.\n\nNature Neuroscience announced updated standards in 2013 editorial and released a reporting checklist authors should complete on submission that requires sample-size planning.\nJournal of Neuroscience has issued updated author guidelines as of March of 2017 that asks for sample-size justification. The updated guidelines are here.\n\n\nEthical guidelines - the American Stasitical Association has put forth ethical guidelines for those who regularly use statistics. These enjoin statisticians to collect neither too much nor too little data (as both are ethically problematic). The guidelines are online here.\n\n\n\nBackground Reading\n\nFor neuroscientists, the most lucid explanation of the importance of sample-size planning is a recent commentary by Yarkoni (2009). This paper explains why small sample sizes are problematic even if results are statistically significant.\n\nYarkoni, T. (2009). Big Correlations in Little Studies. Perspectives on Psychological Science, 4(3), 294–298. https://doi.org/10.1111/j.1745-6924.2009.01127.x\n\nThe fact that sample sizes are too small in the neurosciences is now well-documented. Here are three eye-opening readings:\n\nButton, K. S., Ioannidis, J. P. a., Mokrysz, C., Nosek, B. a., Flint, J., Robinson, E. S. J., & Munafò, M. R. (2013). Power failure: why small sample size undermines the reliability of neuroscience. Nature Reviews. Neuroscience, 14(5), 365–76. https://doi.org/10.1038/nrn3475\nSzucs, D., & Ioannidis, J. P. A. (2017). When Null Hypothesis Significance Testing Is Unsuitable for Research: A Reassessment. Frontiers in Human Neuroscience, 11. https://doi.org/10.3389/fnhum.2017.00390\nCarniero et al. (currently a pre-print). Effect sizes and statistical power in the rodent fear conditioning liturature: A systematic review. http://dx.doi.org/10.1101/116202\n\nRun-and-check is a common practice, but not a good one. Here’s a modern source and a classic source:\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–66. https://doi.org/10.1177/0956797611417632\nAnscombe, F. J. (1954). Fixed-Sample-Size Analysis of Sequential Observations. Biometrics, 10(1), 89. https://doi.org/10.2307/3001665\n\nUnderstanding effect sizes can be challenging. Here’s an excellent source that makes everything clear:\n\nLakens, D. (2013). Calculating and reporting effect sizes to facilitate cumulative science: A practical primer for t-tests and ANOVAs. Frontiers in Psychology, 4(NOV), 1–12. https://doi.org/10.3389/fpsyg.2013.00863\n\nFinally, here are some other sources that are well-worth checking out:\n\nIoannidis, J. P. A. (2005). Why Most Published Research Findings Are False. PLoS Medicine, 2(8), e124. https://doi.org/10.1371/journal.pmed.0020124\nCumming, G. (2008). Replication and p intervals. Perspectives on Psychological Science, 3(4), 286–300. https://doi.org/10.1111/j.1745-6924.2008.00079.x\n\n\n\n\nPlanning for Power\n\nDealing with Uncertainty and Publication Bias\nThere are already lots of sources and tools for planning for power. Although the approach is easy to adopt, it is important to remember that:\n\nEffect sizes in the published literature may be biased, and\nEffect sizes estimated from small samples are often uncertain.\n\nTherefore, it is a good idea to hedge your sample-size estimatesagainst both bias and uncertainty.\nKen Kelley’s group has an approach that does this:\n\nThe R package of tools is called BUCSS– Bias and Unertainty Corrected Sample Sizes: https://cran.r-project.org/web/packages/BUCSS/index.html\nThe website designingexperiments.com has web apps that allow you to plan for power in this careful way without having to learn R. Scroll to the bottom of the page of webapps to select your design and load the appropriate web app.\nA paper describing this approach is here: Anderson, S. F., Kelley, K., & Maxwell, S. E. (2017). Sample-Size Planning for More Accurate Statistical Power: A Method Adjusting Sample Effect Sizes for Publication Bias and Uncertainty. Psychological Science, 95679761772372. https://doi.org/10.1177/0956797617723724\n\n\n\nSequential Testing\nIf you are going to use planning for power, sequential testing can be more efficient, especially in the exploratory phase of research. Lakens offers an excellent tutorial:\n\nLakens, D. (2014). Performing high-powered studies efficiently with sequential analyses. European Journal of Social Psychology, 44(7), 701–710. https://doi.org/10.1002/ejsp.2023\n\n\n\n\nPlanning for Precision\nPlanning for precision is also known as Accuracy in Parameter Estimatation (AIPE). In this approach, the researcher’s goal is to control the noise/error in the result–a sample size is selected that will give a reasonable margin of error relative to the research question and the scale of measurement.\nOne set of readings and tools are from Geoff Cumming and his collaborators:\n\nesci - Is a free set of online tools for understanding as well as an R package for data analysis. Here is a link to planning-for-precision functions in the online tools included in esci: https://esci.thenewstatistics.com/esci-precision.html#tab-1\nCumming has two books which have coverage on planning for precision. Both are readable and accessible to a general scientific audience.\n\nThis book is for those already well-versed in p values: Cumming, G. (2011). Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis. New York: Routledge.\nThis book is for undergraduates learning stats, but has a bit more updated material on planning for precision: Cumming, G., & Calin-Jageman, R. J. (2024, 2nd edition). Introduction to the new statistics: Estimation, open science, and beyond. New York: Routledge.\n\n\nAnother set of readings and tools are from Ken Kelley and his collaborators.\n\nMBESS - Is a free R package that contains many different useful functions. Among those are functions for planning for precision (which Kelley terms AIPE). The functions in MBESS are complex, but they can be used for a wide variety of experimental designs. https://cran.r-project.org/web/packages/MBESS/index.html\nDesigningExperiments.com has free web applications that allow planning for precision with the functions embedded into MBESS. This makes them easier to use. Given that they can handle complex designs, it is not surprising that the learning curve is a bit steep even for the web application.\nKelley and his colleagues have a number of papers and sources on the AIPE approach. Also reccomended is his excellent book Designing Experiments and Analyzing Data.\n\nMaxwell, S. E., Delaney, H. D., & Kelley, K. (2018). Designing Experiments and Analyzing Data: A Model Comparison Perspective (3rd ed.). New York: Routledge.\nMaxwell, S. E., Kelley, K., & Rausch, J. R. (2008). Sample Size Planning for Statistical Power and Accuracy in Parameter Estimation. Annual Review of Psychology, 59(1), 537–563. https://doi.org/10.1146/annurev.psych.59.103006.093735\nKelley, K. (2007). Sample size planning for the coefficient of variation from the accuracy in parameter estimation approach. Behav Res Meth, 39(4), 755–766. https://doi.org/10.3758/BF03192966\nKelley, K., & Maxwell, S. E. (2003). Sample Size for Multiple Regression: Obtaining Regression Coefficients That Are Accurate, Not Simply Significant. Psychological Methods, 8(3), 305–321. https://doi.org/10.1037/1082-989X.8.3.305\nKelley, K., & Maxwell, S. E. (2003). Sample Size for Multiple Regression: Obtaining Regression Coefficients That Are Accurate, Not Simply Significant. Psychological Methods, 8(3), 305–321. https://doi.org/10.1037/1082-989X.8.3.305\n\n\nSAS has tools that enable planning for precision:\n\nThe usage guide is here.\nSAS also provides some sample cases, such as this one.\n\nPlanning for precision is also perfect for Bayesians. John Kruschke has written a book and provides excellent tools for what her terms the Bayesian New Statistics:\n\nKruschke, J (2014). Doing Bayesian Data Analysis. Eslevier. https://www.elsevier.com/books/doing-bayesian-data-analysis/kruschke/978-0-12-405888-0\nKruschke, J. K., & Liddell, T. M. (2017). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. Psychonomic Bulletin & Review. https://doi.org/10.3758/s13423-016-1221-4\n\n\n\nOther Approaches to Planning\nThere are lots of other good ways to plan your studies..I couldn’t cover them all. Here are three noteworthy papers and approaches.\n\nPlanning for Evidence - If you like Bayesian hypothesis testing, a very good approach is to plan for evidence rather than sample size. That is, you can commit to collecting data until you achieve clear evidence for your hypothesis or for the null hypothesis. It sounds scary because data collection is therefore open-ended, yet simulations show this can actually be a very efficient approach.\n\nSchönbrodt, F. D., & Wagenmakers, E.-J. (2017). Bayes factor design analysis: Planning for compelling evidence. Psychonomic Bulletin & Review, 1–16. https://doi.org/10.3758/s13423-017-1230-y\n\n\nPlanning for Stability - not too different from planning for precision, this approach is to select a sample-size that will provide enough information that subsequent replications will achieve similar results within a set level of similarity.\n\nLakens, D., & Evers, E. R. K. (2014). Sailing From the Seas of Chaos Into the Corridor of Stability: Practical Recommendations to Increase the Informational Value of Studies. Perspectives on Psychological Science, 9(3), 278–292. https://doi.org/10.1177/1745691614528520\nGelman’s approach:\n\nGelman, A., & Carlin, J. (2014). Beyond power calculations: Assessing Type S (sign) and Type M (magnitude) errors. https://doi.org/10.1177/1745691614551642",
    "crumbs": [
      "Course",
      "Resources"
    ]
  },
  {
    "objectID": "What Not To Do.html",
    "href": "What Not To Do.html",
    "title": "How Not to Get Your Sample Size",
    "section": "",
    "text": "The video above walks through some common but poor sample-size planning practices: run-and-check (backing into a sample-size by chasing statistical significance) and uncritically adopting previous sample sizes.\nIn the explorations below you can dig into why run-and-check is so bad, and you can also explore the suprising fact that p &lt; .05 does not mean you have an adequate sample size.",
    "crumbs": [
      "Course",
      "What Not To Do"
    ]
  },
  {
    "objectID": "What Not To Do.html#exploration-dont-run-and-check",
    "href": "What Not To Do.html#exploration-dont-run-and-check",
    "title": "How Not to Get Your Sample Size",
    "section": "Exploration: Don’t Run-and-Check!",
    "text": "Exploration: Don’t Run-and-Check!\nA common approach to sample-size determination is Run-and-Check: you run a small batch of samples, check for significance, and then keep adding samples until you either get what you want or run out of time/resources. Sometimes run-and-check is conducted over people/labs – keep assigning the same project to different trainees until someone with ‘good hands’ gets it to work (sometimes that really is good hands, but it can also easily be capitalization chance!).\nLet’s examine why run-and-check is so dangerous to good science. We’ll simulate data for a two-group experiment, with the simulation set so that there is no true effect in the population. We’ll start with n = 3/group, then check. If the results are significant, we stop, otherwise we add 1 sample/group and check again, and so on, to a limit of n = 30 group. In this scenario, any statistically significant finding is a false positive. We’ll run 10,000 simulations and check the false-positive rate.\nThe code for the simulation is below. Before you scroll down to its output, what is your guess for the false positive rate for this run-and-check approach?\n\n# If needed, install needed libraries\nif (!require(\"statpsych\")) install.packages(\"statpsych\")\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif (!require(\"gganimate\")) install.packages(\"gganimate\")\nif (!require(\"gifski\")) install.packages(\"gifski\")\n\n\n# Load needed libraries\nlibrary(statpsych)\nlibrary(ggplot2)\nlibrary(gganimate)\n\n\n# Simulation parameters\nn_initial &lt;- 3\nn_add_per_round &lt;- 1\nn_to_give_up &lt;- 30\ntrue_effect_size &lt;- 0\ntotal_sims &lt;- 1000\n\n# Initialize variables for results\nfalse_positives &lt;- 0\nrandc_res &lt;- data.frame(p = NA, n = NA, sample = NA, eventually_significant = NA)\nresults_row &lt;- 0\n\n# simulation \nfor (i in 1:total_sims) {\n  \n  # simulate data at initial sample size for both groups and do t.test\n  n_current &lt;- n_initial\n  g1 &lt;- rnorm(n_current, 0, 1)\n  g2 &lt;- rnorm(n_current, 0 + true_effect_size, 1)\n  res &lt;- t.test(g1, g2)$p.value\n  results_row &lt;- results_row + 1\n  \n  # store the results for plotting\n  randc_res[results_row, ] &lt;- c(\n    res,\n    n_current,\n    i,\n    FALSE\n  )\n  \n  # if not significant and not at n_to_give_up yet, keep going\n  while (res &gt;= .05 & n_current &lt; n_to_give_up) {\n    g1 &lt;- c(g1, rnorm(n_add_per_round, 0, 1))\n    g2 &lt;- c(g2, rnorm(n_add_per_round, 0 + true_effect_size, 1))\n    n_current &lt;- n_current + n_add_per_round\n    results_row &lt;- results_row + 1\n    res &lt;- t.test(g1, g2)$p.value\n    \n    randc_res[results_row, ] &lt;- c(\n      res,\n      n_current,\n      i,\n      FALSE\n    )\n  }\n  \n  # If the final result is significnt, that's a false positive (assuming true_effect_size == 0)\n  if (res &lt; .05) {\n    false_positives &lt;- false_positives + 1\n    randc_res[randc_res$sample == i, ]$eventually_significant &lt;- TRUE\n  }\n  \n}\n\n\nprint(\n  paste(\n    \"Simulated\", total_sims, \n    \"two-group studies with a true mean difference of\", true_effect_size,\n    \"where sample sizes started off with\", n_initial, \"/group\",\n    \"and if not significant added\", n_add_per_round, \"/group and re-checked,\",\n    \"up to significance or a max sample size of\", n_to_give_up, \".\"\n  )\n)\nprint(\"Proportion of significant findings\")\nprint(false_positives / total_sims)\n\n\nSimulation Results: False-Positive Rate\nThe code simulated 1000 two-group studies with a true mean difference 0 where sample sizes started off with 3/group and if not significant added 1/group up to significance or a max sample size of 30.\nUnder this scenario, the proportion of false positives is: 22.1%.\nThis scenario is a bit extreme, but it makes it clear that run-and-check is not a good method for determining sample size.\n\n\nSimulation Results: The Deceptive Dance of the p Value\nTo make this even more clear, we’re going to plot the p values as sample sizes are added, cycling through the first 10 simulations(Figure 1). What you’ll notice is that the p values “feel” like they are following trajectories, swining higher or lower. This can give the researcher using run-and-check the feeling that they’re on to something, and that they are justified adding more samples, or worse, adjusting their analyses to coax the p value below .05.\n\n\n\n\n\n\n\n\nFigure 1: Evolution of p values as samples are added but no true effect\n\n\n\n\n\nThus, it’s not just the instinsic false-positive rate of run-and-check that is dangerous but also the false impression it can give that might license analytic flexibility towards erroneous results. For more on the perils of run-and-check see (Anscombe 1954; Simmons, Nelson, and Simonsohn 2011).",
    "crumbs": [
      "Course",
      "What Not To Do"
    ]
  },
  {
    "objectID": "What Not To Do.html#questions",
    "href": "What Not To Do.html#questions",
    "title": "How Not to Get Your Sample Size",
    "section": "Questions",
    "text": "Questions\n\nJoe doesn’t think he needs this course. He just runs a small batch of animals for each study, and then if results look interesting he adds samples to get to p &lt; .05. What should we make of this approach? &lt;div class=‘webex-radiogroup’ id=‘radio_HUEMDYLVBK’&gt;&lt;label&gt;&lt;input type=“radio” autocomplete=“off” name=“radio_HUEMDYLVBK” value=““&gt;&lt;/input&gt; &lt;span&gt;This is a biased approach that capitilizes on chance&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=”radio” autocomplete=“off” name=“radio_HUEMDYLVBK” value=“answer”&gt;&lt;/input&gt; &lt;span&gt;Smart work, Joe, no wonder you always confirm your hypotheses!&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;\nAnisha wants to run a single-cell transcriptomics experiment. She finds a paper that runs 3 biological replicates. She decides to use the same sample size. What should we make of this approach? &lt;div class=‘webex-radiogroup’ id=‘radio_UMDKQDHBFO’&gt;&lt;label&gt;&lt;input type=“radio” autocomplete=“off” name=“radio_UMDKQDHBFO” value=““&gt;&lt;/input&gt; &lt;span&gt;Sadly, the published literature is not always a good guide -- many fields regularly use inadquate sample sizes; Anisha needs more information to know if this is reasonable&lt;/span&gt;&lt;/label&gt;&lt;label&gt;&lt;input type=”radio” autocomplete=“off” name=“radio_UMDKQDHBFO” value=“answer”&gt;&lt;/input&gt; &lt;span&gt;Sweet; as long as you have a citation to back you up, you're fine!&lt;/span&gt;&lt;/label&gt;&lt;/div&gt;",
    "crumbs": [
      "Course",
      "What Not To Do"
    ]
  }
]